{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFrL6CYb_nog"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28Dg53L3-FN1",
        "outputId": "7804aeae-549c-4694-917f-4be3a75e5512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Library langchain version: 0.3.23\n",
            "Library langchain_community not found. Installing...\n",
            "Library huggingface_hub version: 0.30.2\n",
            "Library langchain_openai not found. Installing...\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "libraries = [\"langchain\", \"langchain_community\", \"huggingface_hub\", \"langchain_openai\"]\n",
        "\n",
        "for library in libraries:\n",
        "    try:\n",
        "        # Try to import the library\n",
        "        module = importlib.import_module(library)\n",
        "        print(f\"Library {library} version: {module.__version__}\")\n",
        "    except ImportError:\n",
        "\n",
        "        # If library is not installed, attempt to install it\n",
        "        print(f\"Library {library} not found. Installing...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", library])\n",
        "\n",
        "        # After installing, import again and print the version\n",
        "        module = importlib.import_module(library)\n",
        "        # print(f\"Library {library} version after installation: {module.__version__}\")\n",
        "    except AttributeError:\n",
        "        # If library doesn't have __version__ attribute\n",
        "        print(f\"Library {library} does not have a __version__ attribute.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u6TKBn32neW",
        "outputId": "6b5e46d2-911e-4380-be9a-6293829f9ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcdqBAic-t0s"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('HF_TOKEN_NEW')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "VHqg17i9yUJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9c1SAvpSgrk"
      },
      "source": [
        "#LLM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8OPmHwl5TNW"
      },
      "source": [
        "##Azure Open AI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT6UPs0h5b4I"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureOpenAI\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2024-05-01-preview\"\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get('AZ_OPENAI_KEY')\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] =  \"https://azopenai-demo.openai.azure.com/\"\n",
        "\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to HuggingFace LLM"
      ],
      "metadata": {
        "id": "m-Vhhx8xyG4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LLaMA 4 model from Hugging Face Hub (make sure it's a chat-compatible LLaMA4 model)\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN_NEW')\n",
        "llm_hf = HuggingFaceHub(\n",
        "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",  # Adjust if you're using LLaMA 4 when it's available\n",
        "    model_kwargs={\n",
        "        \"temperature\": 0.7,\n",
        "        \"max_new_tokens\": 256,\n",
        "        \"top_p\": 0.9,\n",
        "        \"repetition_penalty\": 1.1\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "xHHj3xJsyC-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "967f0782-2d88-4a0c-af59-bdea38f09360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-9faf515431b5>:3: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  llm_hf = HuggingFaceHub(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7"
      ],
      "metadata": {
        "id": "yfKGIQ-OLgS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis and Classification"
      ],
      "metadata": {
        "id": "rtpW6EMK1J50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Simple sentiment analysis chain\n",
        "sentiment_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Analyze the sentiment of the following text. Respond with only 'positive', 'negative', or 'neutral'.\\n\\nText: {text}\"\n",
        ")\n",
        "\n",
        "sentiment_chain = sentiment_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Example usage\n",
        "text = \"This is a great product! I love it.\"\n",
        "sentiment = sentiment_chain.invoke({\"text\": text})\n",
        "print(f\"Sentiment: {sentiment}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qov1m_4mLrKD",
        "outputId": "bad7639b-b468-4cbf-cdea-fe887f869323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: \n",
            "\n",
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Text Classifier\n"
      ],
      "metadata": {
        "id": "cGzaHEW9CDoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Multi-class classification template\n",
        "classification_template = \"\"\"\n",
        "Classify the following text into one of these categories:\n",
        "- Product Question\n",
        "- Technical Support\n",
        "- Billing Issue\n",
        "- Feature Request\n",
        "- Complaint\n",
        "Text: {text}\n",
        "\n",
        "Classification (respond with all the categories as per the text. You may show more than 1 if the text falls under multiple classess.):\n",
        "\"\"\"\n",
        "\n",
        "classification_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=classification_template\n",
        ")\n",
        "\n",
        "classifier_chain = LLMChain(llm=llm, prompt=classification_prompt)\n",
        "\n",
        "# llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "# Example usage\n",
        "category = classifier_chain.invoke(\"I am unable to login and My subscription was charged twice this month.\")\n",
        "print(f\"Category: {category}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkCzkXnINqZr",
        "outputId": "41613e4f-d46b-44e1-f728-80a3b2df76b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category: {'text': '\\n- Technical Support\\n- Billing Issue'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-74975a5ec2f2>:22: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  classifier_chain = LLMChain(llm=llm, prompt=classification_prompt)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Classification with Structured Output\n"
      ],
      "metadata": {
        "id": "8m1Tr5tGCJkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Dict\n",
        "# Use built-in float instead of typing.Float\n",
        "\n",
        "class ClassificationResult(BaseModel):\n",
        "    primary_category: str = Field(description=\"The main category of the text\")\n",
        "    confidence: float = Field(description=\"Confidence score between 0 and 1\") # Changed typing.Float to float\n",
        "    secondary_categories: List[Dict[str, float]] = Field( # Changed typing.Float to float\n",
        "        description=\"Other possible categories with confidence scores\"\n",
        "    )\n",
        "\n",
        "classification_parser = PydanticOutputParser(pydantic_object=ClassificationResult)\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "advanced_classification_prompt = PromptTemplate(\n",
        "    template=\"Classify the following text:\\n{text}\\n{format_instructions}\",\n",
        "    input_variables=[\"text\"],\n",
        "    partial_variables={\"format_instructions\": classification_parser.get_format_instructions()}\n",
        ")\n",
        "\n",
        "advanced_classifier = advanced_classification_prompt | llm | classification_parser"
      ],
      "metadata": {
        "id": "OXcJ7e5UO4L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "user_input = \"I'm having trouble logging into my account. I think my password might be incorrect.\"\n",
        "\n",
        "# Invoke the advanced classifier\n",
        "result = advanced_classifier.invoke({\"text\": user_input})\n",
        "\n",
        "# Print the classification results\n",
        "print(result)\n",
        "\n",
        "# Access specific fields of the result\n",
        "print(f\"Primary Category: {result.primary_category}\")\n",
        "print(f\"Confidence: {result.confidence}\")\n",
        "print(f\"Secondary Categories: {result.secondary_categories}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zL_8gVQPRRc",
        "outputId": "7ad38b26-8124-4418-e6ee-7969f240a333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "primary_category='IT' confidence=0.9 secondary_categories=[{'security': 0.7}, {'technology': 0.6}]\n",
            "Primary Category: IT\n",
            "Confidence: 0.9\n",
            "Secondary Categories: [{'security': 0.7}, {'technology': 0.6}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrating Traditional ML Models\n"
      ],
      "metadata": {
        "id": "VSRYyGWrCSBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "# Training data\n",
        "texts = [\"I love this product\", \"This doesn't work\", \"How do I install this?\"]\n",
        "labels = [\"positive\", \"negative\", \"question\"]\n",
        "\n",
        "# Create a scikit-learn pipeline\n",
        "ml_classifier = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "ml_classifier.fit(texts, labels)\n",
        "# Wrap in a LangChain tool\n",
        "from langchain.tools import Tool\n",
        "\n",
        "def classify_text(text):\n",
        "    pred = ml_classifier.predict([text])[0]\n",
        "    proba = ml_classifier.predict_proba([text])[0]\n",
        "    confidence = np.max(proba)\n",
        "    return {\"classification\": pred, \"confidence\": float(confidence)}\n",
        "\n",
        "classification_tool = Tool(\n",
        "    name=\"TextClassifier\",\n",
        "    func=classify_text,\n",
        "    description=\"Classifies text as positive, negative, or question\"\n",
        ")"
      ],
      "metadata": {
        "id": "Z5YRo7FrQCva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#example usage\n",
        "text_to_classify = \"This is an awesome product!\"\n",
        "result = classification_tool.run(text_to_classify)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_71saftcQRmt",
        "outputId": "5a1c6a48-2df5-4185-a0fd-8338b7c5fa61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'classification': np.str_('positive'), 'confidence': 0.44264572877572594}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the GPT-4 LLM and setting it up\n",
        "os.environ['OPENAI_API_KEY']= \"<Use OpenAI API Key>\"\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"<Use OpenAI URL>\""
      ],
      "metadata": {
        "id": "HwhboEsS25br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Selection Strategies"
      ],
      "metadata": {
        "id": "GdRZl9yaCYIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.evaluation.criteria import LabeledCriteriaEvalChain, Criteria\n",
        "from langchain_openai import ChatOpenAI\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_models_on_task(models, task_examples, evaluation_criteria, evaluator_llm):\n",
        "    results = []\n",
        "\n",
        "    evaluator = LabeledCriteriaEvalChain.from_llm(\n",
        "        llm=evaluator_llm,\n",
        "        criteria=evaluation_criteria\n",
        "    )\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        chain = LLMChain(llm=model, prompt=task_examples[\"prompt\"])\n",
        "\n",
        "        scores = []\n",
        "        for example in task_examples[\"examples\"]:\n",
        "            prediction = chain.invoke(example[\"input\"])\n",
        "\n",
        "            evaluation_result = evaluator.evaluate_strings(\n",
        "                input=example[\"input\"][\"text\"],\n",
        "                prediction=prediction,\n",
        "                reference=example[\"expected\"]\n",
        "            )\n",
        "\n",
        "            scores.append(evaluation_result.get(\"score\", 0))\n",
        "\n",
        "        results.append({\n",
        "            \"model\": model_name,\n",
        "            \"avg_score\": sum(scores) / len(scores),\n",
        "            \"min_score\": min(scores),\n",
        "            \"max_score\": max(scores)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "llm_gpt4o_mini = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "models = {\n",
        "    \"gpt-3.5-turbo\": llm,\n",
        "    \"gpt-4o-mini\": llm_gpt4o_mini,\n",
        "}\n",
        "\n",
        "# Define classification task\n",
        "classification_examples = {\n",
        "    \"prompt\": PromptTemplate(\n",
        "        template=\"Classify the sentiment: {text}\",\n",
        "        input_variables=[\"text\"]\n",
        "    ),\n",
        "    \"examples\": [\n",
        "        {\"input\": {\"text\": \"I love this product\"}, \"expected\": \"positive\"},\n",
        "        {\"input\": {\"text\": \"This is terrible\"},    \"expected\": \"negative\"},\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Run evaluation using gpt-3.5-turbo as the evaluator\n",
        "results = evaluate_models_on_task(\n",
        "    models=models,\n",
        "    task_examples=classification_examples,\n",
        "    evaluation_criteria=Criteria.CORRECTNESS,\n",
        "    evaluator_llm=llm\n",
        ")\n",
        "\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "EDr1J_h40epW",
        "outputId": "c7b2c916-fc7c-471d-e3b3-63a425782fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e51a2cb734ef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m llm_gpt4o_mini = ChatOpenAI(\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o-mini\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m  \u001b[0;31m# noqa: D419\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    656\u001b[0m                 )\n\u001b[1;32m    657\u001b[0m             \u001b[0msync_specific\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"http_client\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mclient_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msync_specific\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             raise OpenAIError(\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0;34m\"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             )\n",
            "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9fyeIQB03oKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "\n",
        "# Replace 'your_file_id' with the actual file ID from the Google Drive URL\n",
        "file_id = \"1mPP2UvJAkGjqF78rLf8S1izVoE7-5dfl\"\n",
        "download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "data_for_fine_tuning = pandas.read_csv(download_url)"
      ],
      "metadata": {
        "id": "CPSa6xQ8qhOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_for_fine_tuning.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5W_z53hz7rPM",
        "outputId": "4583db9b-2593-4a97-9179-c26abc9c3066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               query  \\\n",
              "0   I can't log into my account after three attempts   \n",
              "1                   How do I cancel my subscription?   \n",
              "2                   When will my order be delivered?   \n",
              "3  The mobile app keeps crashing when I try to up...   \n",
              "4                    Do you offer student discounts?   \n",
              "\n",
              "                                            response           category  \\\n",
              "0  Your account may be temporarily locked for sec...     Authentication   \n",
              "1  You can cancel your subscription by logging in...            Billing   \n",
              "2  Thank you for your patience. Based on your ord...            Billing   \n",
              "3  I'm sorry you're experiencing this issue. Plea...  Technical Support   \n",
              "4  Yes! We offer a 15% discount for verified stud...      StudentBeans.   \n",
              "\n",
              "  priority  \n",
              "0     High  \n",
              "1   Medium  \n",
              "2   Medium  \n",
              "3     High  \n",
              "4  Pricing  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-02ccd356-16d5-40d5-b162-c93c3df39e29\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query</th>\n",
              "      <th>response</th>\n",
              "      <th>category</th>\n",
              "      <th>priority</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I can't log into my account after three attempts</td>\n",
              "      <td>Your account may be temporarily locked for sec...</td>\n",
              "      <td>Authentication</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How do I cancel my subscription?</td>\n",
              "      <td>You can cancel your subscription by logging in...</td>\n",
              "      <td>Billing</td>\n",
              "      <td>Medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>When will my order be delivered?</td>\n",
              "      <td>Thank you for your patience. Based on your ord...</td>\n",
              "      <td>Billing</td>\n",
              "      <td>Medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The mobile app keeps crashing when I try to up...</td>\n",
              "      <td>I'm sorry you're experiencing this issue. Plea...</td>\n",
              "      <td>Technical Support</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Do you offer student discounts?</td>\n",
              "      <td>Yes! We offer a 15% discount for verified stud...</td>\n",
              "      <td>StudentBeans.</td>\n",
              "      <td>Pricing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02ccd356-16d5-40d5-b162-c93c3df39e29')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-02ccd356-16d5-40d5-b162-c93c3df39e29 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-02ccd356-16d5-40d5-b162-c93c3df39e29');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-80009020-7416-4afe-acd6-d44ca9246fbc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-80009020-7416-4afe-acd6-d44ca9246fbc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-80009020-7416-4afe-acd6-d44ca9246fbc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_for_fine_tuning",
              "summary": "{\n  \"name\": \"data_for_fine_tuning\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"How do I cancel my subscription?\",\n          \"Do you offer student discounts?\",\n          \"When will my order be delivered?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"You can cancel your subscription by logging into your account\",\n          \"Yes! We offer a 15% discount for verified students. To activate your student discount\",\n          \"Thank you for your patience. Based on your order number\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Billing\",\n          \" StudentBeans.\",\n          \"Authentication\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"priority\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"High\",\n          \"Medium\",\n          \"Pricing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Fine-tuning Data with LangChain\n"
      ],
      "metadata": {
        "id": "CAavc36BCjNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Google Drive file ID and download URL\n",
        "file_id = \"1mPP2UvJAkGjqF78rLf8S1izVoE7-5dfl\"\n",
        "download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "# Load the CSV using pandas\n",
        "data_df = pd.read_csv(download_url)\n",
        "\n",
        "# Function to prepare data in OpenAI fine-tuning format\n",
        "def prepare_openai_fine_tuning_data(df, instruction, input_column=\"query\", response_column=\"response\"):\n",
        "    formatted_data = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        # Check if response exists\n",
        "        if pd.isna(row[response_column]):\n",
        "            print(f\"Warning: Missing response for query: {row[input_column]}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        example = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": instruction},\n",
        "                {\"role\": \"user\", \"content\": row[input_column]},\n",
        "                {\"role\": \"assistant\", \"content\": row[response_column]}\n",
        "            ]\n",
        "        }\n",
        "        formatted_data.append(example)\n",
        "\n",
        "    return formatted_data\n",
        "\n",
        "# Instruction for the model\n",
        "instruction = \"You are a customer support assistant. Provide helpful, accurate, and concise responses.\"\n",
        "\n",
        "# Generate fine-tuning data\n",
        "fine_tuning_data = prepare_openai_fine_tuning_data(\n",
        "    data_df,\n",
        "    instruction,\n",
        "    input_column=\"query\",\n",
        "    response_column=\"response\"\n",
        ")\n",
        "\n",
        "# Save as JSONL\n",
        "with open(\"fine_tuning_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in fine_tuning_data:\n",
        "        f.write(json.dumps(item) + \"\\n\")"
      ],
      "metadata": {
        "id": "Rfxb9TTJSIpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ft_data = json.loads(\"\")\n",
        "import json\n",
        "\n",
        "# Correct way to load a JSONL file\n",
        "file_path = \"/content/fine_tuning_data.jsonl\"\n",
        "\n",
        "docs = []\n",
        "with open(file_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        doc = json.loads(line)\n",
        "        docs.append(doc)\n",
        "\n",
        "# Now you can print some examples\n",
        "for doc in docs[:2]:\n",
        "    print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_xtwIbXK5Kf",
        "outputId": "643c422a-9b89-4f94-83a8-b1f2f00517d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'messages': [{'role': 'system', 'content': 'You are a customer support assistant. Provide helpful, accurate, and concise responses.'}, {'role': 'user', 'content': \"I can't log into my account after three attempts\"}, {'role': 'assistant', 'content': \"Your account may be temporarily locked for security. Please use the 'Forgot Password' link on the login page to reset your password or wait 30 minutes before trying again. If the issue persists contact our security team at security@example.com.\"}]}\n",
            "{'messages': [{'role': 'system', 'content': 'You are a customer support assistant. Provide helpful, accurate, and concise responses.'}, {'role': 'user', 'content': 'How do I cancel my subscription?'}, {'role': 'assistant', 'content': 'You can cancel your subscription by logging into your account'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade transformers -q"
      ],
      "metadata": {
        "id": "dj0G5MpaMgc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "import json\n",
        "import torch\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# Load the dataset\n",
        "data_files = {\"train\": \"/content/fine_tuning_data.jsonl\"}\n",
        "dataset = load_dataset(\"json\", data_files=data_files)\n",
        "\n",
        "# Prepare prompt-response pairs\n",
        "def process_messages(example):\n",
        "    system_prompt = \"\"\n",
        "    user_input = \"\"\n",
        "    assistant_response = \"\"\n",
        "\n",
        "    for message in example[\"messages\"]:\n",
        "        if message[\"role\"] == \"system\":\n",
        "            system_prompt = message[\"content\"]\n",
        "        elif message[\"role\"] == \"user\":\n",
        "            user_input = message[\"content\"]\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            assistant_response = message[\"content\"]\n",
        "\n",
        "    # Combine into a single training sample\n",
        "    full_prompt = f\"System: {system_prompt}\\nUser: {user_input}\\nAssistant: \"\n",
        "    full_completion = assistant_response\n",
        "\n",
        "    return {\n",
        "        \"input_text\": full_prompt,\n",
        "        \"output_text\": full_completion\n",
        "    }\n",
        "\n",
        "# Apply processing\n",
        "processed_dataset = dataset.map(process_messages, remove_columns=[\"messages\"])\n",
        "\n",
        "# Tokenize\n",
        "def tokenize_function(examples):\n",
        "    inputs = [i + o for i, o in zip(examples[\"input_text\"], examples[\"output_text\"])]\n",
        "    model_inputs = tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=512)\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = processed_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    # evaluation_strategy=\"no\",\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    fp16=torch.cuda.is_available(),\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save\n",
        "model.save_pretrained(\"./gpt2-finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt2-finetuned\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "BnV52fxfKINF",
        "outputId": "141d37d5-482a-4443-eddb-3823493ed071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmnitin3\u001b[0m (\u001b[33mmnitin3-testbook\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250419_040050-m9urcr5q</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mnitin3-testbook/huggingface/runs/m9urcr5q' target=\"_blank\">./gpt2-finetuned</a></strong> to <a href='https://wandb.ai/mnitin3-testbook/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mnitin3-testbook/huggingface' target=\"_blank\">https://wandb.ai/mnitin3-testbook/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mnitin3-testbook/huggingface/runs/m9urcr5q' target=\"_blank\">https://wandb.ai/mnitin3-testbook/huggingface/runs/m9urcr5q</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9/9 01:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./gpt2-finetuned/tokenizer_config.json',\n",
              " './gpt2-finetuned/special_tokens_map.json',\n",
              " './gpt2-finetuned/vocab.json',\n",
              " './gpt2-finetuned/merges.txt',\n",
              " './gpt2-finetuned/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "  # 1. Load the fine-tuned GPT-2 model\n",
        "  model_path = \"./gpt2-finetuned\"  # path where your fine-tuned model is saved\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "  # Create a pipeline for text generation\n",
        "  generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_length=512)\n",
        "\n",
        "  # 2. Define the function to handle text generation\n",
        "  def generate_response(input_text):\n",
        "      # Generate the response using the model pipeline\n",
        "      result = generator(input_text, return_full_text=False)\n",
        "      return result[0]['generated_text']\n",
        "\n",
        "  # 3. Example usage\n",
        "  query = \"I can't reset my password\"\n",
        "  response = generate_response(query)\n",
        "\n",
        "  print(\"\\nResponse from fine-tuned model:\\n\")\n",
        "  print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt-l315oWMMq",
        "outputId": "945a776d-e747-4416-d441-f285a5350f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response from fine-tuned model:\n",
            "\n",
            " or anything so I can't open it. Please, please try again. I'm sorry, there was a problem. There was nothing wrong.\n",
            "\n",
            "12/9/2018\n",
            "\n",
            "I used the \"Help\". I have been using \"Login\" (I'm guessing you are a customer support rep?), and I see nothing wrong. Sorry. I asked if they have a free account and they said they do, but no. You can purchase a free trial subscription at the checkout, or by entering your name above. If you'd like to use this for other purposes see the link by clicking on that link. Thank you.\n",
            "\n",
            "12/1/2018\n",
            "\n",
            "Sorry for the inconvenience! Please try again. After a period of time, the download will stop. Please try again.\n",
            "\n",
            "12/2/2018\n",
            "\n",
            "Just finished signing up, and I'm very happy with the experience. I tried a number of things, but my device has nothing to do with battery performance or anything. So, I've tried the app, and it does indeed work. I'm a bit frustrated. And after getting my device back to normal again, my device seems to have an occasional lag. I just tried going to a place that has Amazon Echo to get a voice notification, and it's saying to use it for voice activation but it's so slow. What can I do? There are instructions on the website. Simply sign into your address bar, in-app purchases are a good way to transfer your voice alerts. I have a new phone, and I need to talk to the phone number. I'm guessing you are a customer support rep? Please, please try again. I'm sorry, there was a problem. There was nothing wrong.\n",
            "\n",
            "12/1,2,3,4,5 We had a problem while attempting to connect to our mobile site. After some time I noticed some errors. To solve the problem, my device has no Wi-Fi on either of my home networks. The app doesn't display the Wi-Fi information. I called AWS Services. AWS said there were some issues and they'll be sending out a solution soon. We should keep trying, if not a solution, we're probably getting a bad experience.\n",
            "\n",
            "13/18/2018\n",
            "\n",
            "Thank you for your patience. I'm unable to connect to the website, but I can see a few emails to try again. I'll try another Google account while I finish doing some other things,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8d89tT7cLj79"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
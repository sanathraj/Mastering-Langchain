{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Iae2SzZLhX37"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup"
      ],
      "metadata": {
        "id": "fuWH4HWrzPar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "libraries = [\"langchain\", \"langchain_community\", \"huggingface_hub\", \"langchain_openai\"]\n",
        "\n",
        "for library in libraries:\n",
        "    try:\n",
        "        # Try to import the library\n",
        "        module = importlib.import_module(library)\n",
        "        print(f\"Library {library} version: {module.__version__}\")\n",
        "    except ImportError:\n",
        "        # If library is not installed, attempt to install it\n",
        "        print(f\"Library {library} not found. Installing...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", library])\n",
        "        # After installing, import again and print the version\n",
        "        module = importlib.import_module(library)\n",
        "        # print(f\"Library {library} version after installation: {module.__version__}\")\n",
        "    except AttributeError:\n",
        "        # If library doesn't have __version__ attribute\n",
        "        print(f\"Library {library} does not have a __version__ attribute.\")"
      ],
      "metadata": {
        "id": "PCi1TGx5We9h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "958e5043-f5ee-4e7f-ee90-bf0e675caf0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Library langchain version: 0.3.15\n",
            "Library langchain_community version: 0.3.15\n",
            "Library huggingface_hub version: 0.27.1\n",
            "Library langchain_openai does not have a __version__ attribute.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Azure Open AI"
      ],
      "metadata": {
        "id": "ZZgUTW8Czd2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import AzureOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2024-05-01-preview\"\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get('AZ_OPENAI_KEY')\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] =  \"https://azopenai-demo.openai.azure.com/\"\n",
        "\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "embedding_model = AzureOpenAI(deployment_name=\"dp-text-embedding-ada-002\", model_name=\"text-embedding-ada-002\")"
      ],
      "metadata": {
        "id": "SOu6-vjsgbaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pinecone"
      ],
      "metadata": {
        "id": "OXP4LhTmClpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone_key = userdata.get('pinecone_api_key')\n",
        "pinecone_key"
      ],
      "metadata": {
        "id": "iuxuPKwICov8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "cf0d8b05-e346-49e8-a876-80a0cdeb0b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fe549fc0-0058-4b43-bc21-71e37054dd9c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "import langchain_community\n",
        "print(\"langchain.__version__\", langchain.__version__)\n",
        "print(\"langchain_community.__version__\", langchain_community.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvcQ84I1WjgT",
        "outputId": "90f3b123-d62c-416a-f563-9d633227be0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain.__version__ 0.3.15\n",
            "langchain_community.__version__ 0.3.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output Parser"
      ],
      "metadata": {
        "id": "QgjHzq0rGhDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured output parsing"
      ],
      "metadata": {
        "id": "yO1EMzLUGr2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from langchain_openai import AzureOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "# Define response schemas\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"answer\", description=\"Answer to the user's question\"),\n",
        "    ResponseSchema(name=\"fact\", description=\"An interesting fact about the answer\")\n",
        "]\n",
        "\n",
        "# Initialize the parser\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "# Get formatting instructions for prompts\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "# Example usage with an LLM\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "\n",
        "# Create a prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"What is the powerhouse of the cell? {format_instructions}\",\n",
        "    input_variables=[\"format_instructions\"],\n",
        ")\n",
        "prompt = prompt_template.format(format_instructions=format_instructions)\n",
        "\n",
        "# Create an LLMChain to integrate the parser\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template, output_parser=output_parser)\n",
        "\n",
        "# Run the chain to get structured output\n",
        "structured_output = chain.invoke(input=format_instructions)\n",
        "print(structured_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8K-DE04fdQqg",
        "outputId": "4accdfd5-00cf-4683-fd39-55e0ade8006e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'format_instructions': 'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"answer\": string  // Answer to the user\\'s question\\n\\t\"fact\": string  // An interesting fact about the answer\\n}\\n```', 'text': {'answer': 'The powerhouse of the cell is the mitochondria.', 'fact': 'Mitochondria are responsible for converting food into energy in the form of ATP.'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W85DAn43Gqyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom output parsers for specific data formats"
      ],
      "metadata": {
        "id": "mHgKI4dqGZI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parsing JSON data"
      ],
      "metadata": {
        "id": "1zSTmacfd0vH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import BaseOutputParser\n",
        "import json\n",
        "\n",
        "class JSONOutputParser(BaseOutputParser):\n",
        "    def parse(self, text: str):\n",
        "        try:\n",
        "            return json.loads(text)\n",
        "        except json.JSONDecodeError as e:\n",
        "            raise ValueError(f\"Failed to parse JSON: {e}\")\n",
        "\n",
        "    def get_format_instructions(self) -> str:\n",
        "        return \"Provide the output in a valid JSON format.\"\n",
        "\n",
        "# Usage\n",
        "parser = JSONOutputParser()\n",
        "\n",
        "model_output = '{\"name\": \"Alice\", \"age\": 30}'  # Example raw output\n",
        "parsed_data = parser.parse(model_output)\n",
        "print(parsed_data)"
      ],
      "metadata": {
        "id": "5_jMjvlBI6Hp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ecf222a-eac7-4865-f8cb-927658086c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'Alice', 'age': 30}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling SQL Queries"
      ],
      "metadata": {
        "id": "H1WH6F1tke5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import BaseOutputParser\n",
        "class SQLParser(BaseOutputParser):\n",
        "    def parse(self, text: str):\n",
        "        if not text.lower().startswith(\"select\"):\n",
        "            raise ValueError(\"Invalid SQL query. Expected a SELECT statement.\")\n",
        "        return text.strip()\n",
        "\n",
        "    def get_format_instructions(self) -> str:\n",
        "        return \"Generate a valid SQL SELECT query.\"\n",
        "\n",
        "# Usage\n",
        "parser = SQLParser()\n",
        "\n",
        "model_output = \"SELECT * FROM users WHERE age > 25;\"  # Example raw output\n",
        "parsed_sql = parser.parse(model_output)\n",
        "print(parsed_sql)\n"
      ],
      "metadata": {
        "id": "G0nYOTBlKJyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62b96687-26f0-418d-f038-c368c318bf9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT * FROM users WHERE age > 25;\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Error Handling in Output Parsing"
      ],
      "metadata": {
        "id": "P5meQNFYG7E0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Structured Output Validation"
      ],
      "metadata": {
        "id": "vxM1DECve5eG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import BaseOutputParser\n",
        "from langchain.schema.output_parser import OutputParserException\n",
        "\n",
        "class MyCustomParser(BaseOutputParser):\n",
        "    def parse(self, text: str) -> str:\n",
        "        if not isinstance(text, str):\n",
        "            raise OutputParserException(\"Expected string input\")\n",
        "\n",
        "        if not text.startswith(\"Result:\"):\n",
        "            raise OutputParserException(\"Output must start with 'Result:'\")\n",
        "\n",
        "        return text.split(\"Result:\", 1)[1].strip()\n",
        "\n",
        "# Example usage\n",
        "parser = MyCustomParser()\n",
        "try:\n",
        "    parsed_output = parser.parse(\"Result: This is valid output\")\n",
        "    print(f\"Success: {parsed_output}\")\n",
        "except OutputParserException as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a1s2BTje2BG",
        "outputId": "dda18339-b505-48a3-d74b-102428012d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success: This is valid output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrating Custom Parsers with LangChain"
      ],
      "metadata": {
        "id": "bNaqfIG5khvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With LCEL"
      ],
      "metadata": {
        "id": "ocFrW6MnHPMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI  # Or your preferred LLM\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# Assuming JSONOutputParser is defined as in your previous code\n",
        "\n",
        "# Define the prompt template\n",
        "prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"Extract data in the following format: {format_instructions}\\n{input}\"\n",
        ")\n",
        "\n",
        "# Initialize the model\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "\n",
        "# Build the LCEL chain\n",
        "chain = (\n",
        "    RunnablePassthrough()  # Pass the input through\n",
        "    | prompt_template  # Apply the prompt template\n",
        "    | llm  # Invoke the LLM\n",
        "    | JSONOutputParser()  # Parse the output using your custom parser\n",
        ")\n",
        "\n",
        "# Example usage:\n",
        "input_data = \"Extract the name and age of the person from this text: Sachin is 51 years old.\"\n",
        "output = chain.invoke(\n",
        "    {\"input\": input_data, \"format_instructions\": '{\"name\": \"string\", \"age\": \"integer\"}'}\n",
        ")\n",
        "print(output)  # Parsed JSON data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwmTTL0XkvYV",
        "outputId": "be50f774-7734-4178-b22f-5ddcd5cd4b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'Sachin', 'age': 51}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Without LCEL"
      ],
      "metadata": {
        "id": "8_vralNXHT8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.schema import BaseOutputParser\n",
        "import json\n",
        "from langchain.chat_models import ChatOpenAI  # Example LLM\n",
        "\n",
        "# Define the JSONOutputParser\n",
        "class JSONOutputParser(BaseOutputParser):\n",
        "    def parse(self, text: str):\n",
        "        try:\n",
        "            return json.loads(text)\n",
        "        except json.JSONDecodeError as e:\n",
        "            raise ValueError(f\"Failed to parse JSON: {e}\")\n",
        "\n",
        "    def get_format_instructions(self) -> str:\n",
        "        return \"Provide the output in a valid JSON format.\"\n",
        "\n",
        "# Define the LLM\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "\n",
        "# Define the prompt and chain\n",
        "prompt = PromptTemplate(template=\"Extract data in the following format: {format_instructions}\\n{input}\")\n",
        "chain = LLMChain(prompt=prompt, llm=llm, output_parser=JSONOutputParser())\n",
        "\n",
        "# Input\n",
        "input_data = \"Extract the name and age of the person from this text: Sachin is 51 years old.\"\n",
        "\n",
        "# Run the chain\n",
        "output = chain.run(input=input_data, format_instructions='{\"name\": \"string\", \"age\": \"integer\"}')\n",
        "print(output)  # Parsed JSON data\n"
      ],
      "metadata": {
        "id": "sAORuLiRKN2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7dda068-a482-454d-b598-fa64b41b0dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'Sachin', 'age': 51}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory Components"
      ],
      "metadata": {
        "id": "Iae2SzZLhX37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory in chains"
      ],
      "metadata": {
        "id": "T9pzv4A-halz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# Initialize the OpenAI LLM\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Create a conversation chain with memory - use ConversationChain instead\n",
        "conversation_chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True  # Optional: set to True to see the chain's internal workings\n",
        ")\n",
        "\n",
        "# Interact with the chain\n",
        "print(conversation_chain.predict(input=\"Hi! What is LangChain?\"))\n",
        "print(conversation_chain.predict(input=\"Can you explain its memory components?\"))\n",
        "print(conversation_chain.predict(input=\"What did I just ask you?\"))"
      ],
      "metadata": {
        "id": "OaG3h41HXKV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23ace954-33b1-4443-f1b3-78fb4c82eeba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi! What is LangChain?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Hello! LangChain is a blockchain platform that focuses on language learning and education. It was created by a team of linguists and blockchain experts with the goal of making language learning more accessible, interactive, and personalized. The platform uses a combination of AI technology and user-generated content to provide a comprehensive learning experience for users. It also has a built-in reward system that incentivizes users to actively participate and contribute to the community.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi! What is LangChain?\n",
            "AI:  Hello! LangChain is a blockchain platform that focuses on language learning and education. It was created by a team of linguists and blockchain experts with the goal of making language learning more accessible, interactive, and personalized. The platform uses a combination of AI technology and user-generated content to provide a comprehensive learning experience for users. It also has a built-in reward system that incentivizes users to actively participate and contribute to the community.\n",
            "Human: Can you explain its memory components?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Sure! LangChain utilizes a decentralized memory storage system, where all the data and information related to language learning is stored on the blockchain. This includes user progress, personalized learning materials, and community-generated content. The platform also uses AI algorithms to analyze and optimize this data, making the learning experience more efficient and tailored to each user's needs. Additionally, the memory components of LangChain allow for seamless synchronization across devices, so users can continue their learning journey from anywhere.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi! What is LangChain?\n",
            "AI:  Hello! LangChain is a blockchain platform that focuses on language learning and education. It was created by a team of linguists and blockchain experts with the goal of making language learning more accessible, interactive, and personalized. The platform uses a combination of AI technology and user-generated content to provide a comprehensive learning experience for users. It also has a built-in reward system that incentivizes users to actively participate and contribute to the community.\n",
            "Human: Can you explain its memory components?\n",
            "AI:  Sure! LangChain utilizes a decentralized memory storage system, where all the data and information related to language learning is stored on the blockchain. This includes user progress, personalized learning materials, and community-generated content. The platform also uses AI algorithms to analyze and optimize this data, making the learning experience more efficient and tailored to each user's needs. Additionally, the memory components of LangChain allow for seamless synchronization across devices, so users can continue their learning journey from anywhere.\n",
            "Human: What did I just ask you?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " You asked me to explain the memory components of LangChain. Did I answer your question satisfactorily?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory in agents"
      ],
      "metadata": {
        "id": "uH-1_2pwFAKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory in Agents\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain_openai import AzureOpenAI\n",
        "\n",
        "# Initialize memory\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "memory = ConversationSummaryMemory(llm=llm)\n",
        "\n",
        "# Define tools for the agent\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Search\",\n",
        "        func=lambda query: f\"Searching for {query}...\",\n",
        "        description=\"Simulates a search engine.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Create an agent with memory\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=\"zero-shot-react-description\",\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Interact with the agent\n",
        "print(agent.run(\"What is LangChain?\"))\n",
        "print(agent.run(\"Can you help me search for examples of memory in LangChain?\"))\n",
        "print(agent.run(\"What did I ask you earlier?\"))  # Refers back to memory\n"
      ],
      "metadata": {
        "id": "LlN_lfLqXh3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f2b46c-708a-4e81-9344-515d0b688497"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use search to look up information on LangChain\n",
            "Action: Search\n",
            "Action Input: LangChain\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mSearching for LangChain...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should read through the top results to gather information\n",
            "Action: Search\n",
            "Action Input: LangChain\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mSearching for LangChain...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should click on the first or second result to get the most reliable information\n",
            "Action: Search\n",
            "Action Input: LangChain\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mSearching for LangChain...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should read through the information on the website to understand what LangChain is\n",
            "Action: Search\n",
            "Action Input: LangChain\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mSearching for LangChain...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m After reading through multiple sources, I now have a better understanding of LangChain and its purpose\n",
            "Final Answer: LangChain is a blockchain-based platform that aims to connect language learners with native speakers for language exchange and learning.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "LangChain is a blockchain-based platform that aims to connect language learners with native speakers for language exchange and learning.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m You should always think about what to do\n",
            "Action: Search\n",
            "Action Input: LangChain + memory + examples\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mSearching for LangChain + memory + examples...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Now I know the final answer\n",
            "Final Answer: There are many examples of memory in LangChain, such as its ability to store and retrieve words and phrases in different languages.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "There are many examples of memory in LangChain, such as its ability to store and retrieve words and phrases in different languages.\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m You probably asked me a question and forgot what it was\n",
            "Action: Search\n",
            "Action Input: \"previous question\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mSearching for previous question...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I see a list of previous questions\n",
            "Action: Select\n",
            "Action Input: first result\u001b[0m\n",
            "Observation: Select is not a valid tool, try one of [Search].\n",
            "Thought:\u001b[32;1m\u001b[1;3m I will use Search\n",
            "Action: Search\n",
            "Action Input: \"previous question\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mSearching for previous question...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I see a list of previous questions\n",
            "Action: Select\n",
            "Action Input: first result\u001b[0m\n",
            "Observation: Select is not a valid tool, try one of [Search].\n",
            "Thought:\u001b[32;1m\u001b[1;3m I will use Search\n",
            "Action: Search\n",
            "Action Input: \"What did I ask you earlier?\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mSearching for What did I ask you earlier?...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I see the original question\n",
            "Final Answer: What did I ask you earlier?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "What did I ask you earlier?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Managing Long-Term Memory and Context"
      ],
      "metadata": {
        "id": "vz4jPr6-ijJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Memory with token limit and language model"
      ],
      "metadata": {
        "id": "YK5jHOYR16vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Initialize memory with token limit and language model\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "memory = ConversationTokenBufferMemory(\n",
        "    llm=llm,  # Add the language model here\n",
        "    max_token_limit=100\n",
        ")\n",
        "\n",
        "# Create the conversation chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory)\n",
        "\n",
        "# Interact with the chain\n",
        "print(conversation.run(\"List memory components in LangChain.\"))\n",
        "print(\"\\n\\n\")\n",
        "print(conversation.run(\"What are the benefits of token buffer memory?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVbnQ-0ZilJh",
        "outputId": "393abfc2-9efc-437f-88c3-183de6bbce29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LangChain is a programming language that is designed for blockchain applications. It is primarily used for smart contracts and decentralized applications. As such, its memory components are optimized for these types of applications. The main memory component in LangChain is the blockchain itself, which stores all the data and transactions in a decentralized and secure manner. In addition, LangChain also utilizes a virtual machine for executing smart contracts, which also contains a memory component for storing and retrieving data. Lastly, LangChain has a built-in storage system for storing larger amounts of data, such as files or images, within the blockchain.\n",
            "\n",
            "\n",
            "\n",
            " Token buffer memory, also known as token bucket memory, is a type of memory used in computer networking and telecommunications. It is used to manage network traffic and ensure that data is transmitted smoothly and efficiently. The main benefit of token buffer memory is that it helps to regulate the flow of data on a network, preventing congestion and ensuring that data is transmitted in a timely manner. Additionally, token buffer memory can also improve network performance by prioritizing and controlling the flow of different types of data. For example, it can give priority to real-time data such as video or audio streams, while limiting the flow of non-essential data such as emails or file downloads. This can lead to a smoother and more reliable network experience for users.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Memory with summarization"
      ],
      "metadata": {
        "id": "Ms78rRMGcT9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# Initialize memory with summarization\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "memory = ConversationSummaryMemory(llm=llm)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "# Interact with the chain\n",
        "print(conversation.run(\"Tell me about LangChain.\"))\n",
        "print(conversation.run(\"What did we discuss earlier?\"))  # Refers to summary\n"
      ],
      "metadata": {
        "id": "O7tBEuewcX3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c03459b0-c3e9-462d-b31f-6fff17a76b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " LangChain is a decentralized platform for language learning. It utilizes blockchain technology to create a secure and transparent environment for users to learn and practice languages. The platform offers a variety of features, including personalized learning plans, virtual classrooms, and language exchange opportunities. Users can also earn rewards and certifications through their progress on the platform. LangChain has partnerships with top language schools and organizations to ensure high-quality content and resources for its users.\n",
            " We were discussing LangChain, a decentralized platform for language learning that utilizes blockchain technology. We talked about its features such as personalized learning plans, virtual classrooms, and language exchange opportunities. We also mentioned that users can earn rewards and certifications on the platform and that it has partnerships with top language schools and organizations. Is there anything else you would like to know about LangChain?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Token-Limited Memory:"
      ],
      "metadata": {
        "id": "01F9SfBQHgf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "\n",
        "# Initialize the OpenAI LLM\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "\n",
        "# Initialize memory\n",
        "memory = ConversationBufferMemory(max_token_limit=100)\n",
        "\n",
        "# Create a conversation chain with memory - use ConversationChain instead\n",
        "conversation_chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "\n",
        ")\n",
        "\n",
        "# Interact with the chain\n",
        "print(conversation.run(\"Explain memory components in LangChain.\"))\n",
        "print(conversation.run(\"What are the benefits of token buffer memory?\"))\n"
      ],
      "metadata": {
        "id": "3lYI9mtsck9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b87f4137-4228-435f-c380-0030e6c0a9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sure, LangChain uses advanced memory components to help users retain and recall new language information more effectively. These components include spaced repetition algorithms, which schedule review sessions based on each user's individual learning curve, and mnemonic devices, which use associations and visual aids to improve memory retention. Additionally, the platform utilizes immersive learning techniques such as virtual reality and interactive games to engage and stimulate the brain for better language retention.\n",
            " Token buffer memory is a critical component of LangChain's advanced memory system. It allows users to store and retrieve tokens related to their language learning progress. This not only helps with organization and tracking of their learning, but also provides motivation and a sense of accomplishment as they earn and accumulate more tokens. Additionally, the token buffer memory is integrated with the spaced repetition algorithms, mnemonic devices, and immersive learning techniques, allowing for a more personalized and effective learning experience.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hybrid Approaches"
      ],
      "metadata": {
        "id": "eANufNPecmKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "# Initialize the OpenAI LLM\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "\n",
        "# Initialize hybrid memory\n",
        "memory = ConversationSummaryBufferMemory(\n",
        "    llm=llm,\n",
        "    max_token_limit=150\n",
        ")\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "# Interact with the chain\n",
        "print(conversation.run(\"Tell me about Artificial Intelligence.\"))\n",
        "print(conversation.run(\"What did we discuss earlier?\"))\n"
      ],
      "metadata": {
        "id": "PwrQf97Tcsjw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2966fd57-b474-42a6-f8fb-1fdcc7c0ad09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Artificial Intelligence, or AI, is a branch of computer science that focuses on creating intelligent machines that can think and act like humans. It involves the development of algorithms and software programs that can learn from data, make decisions, and solve problems on their own. AI is a rapidly growing field and has many applications in various industries, such as healthcare, finance, and transportation. Some examples of AI technologies include machine learning, natural language processing, and computer vision. AI systems can be categorized as either weak AI, which is designed for specific tasks, or strong AI, which aims to replicate human-like intelligence. Are there any specific aspects of AI you would like to know more about?\n",
            " We discussed Artificial Intelligence, its definition and applications in different industries. We also talked about the two types of AI - weak and strong. Is there anything else you would like to know?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings and Vector Stores"
      ],
      "metadata": {
        "id": "6r6ueok7KUyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up an Embedding Model in LangChain"
      ],
      "metadata": {
        "id": "fn3sj78gLXPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings, AzureOpenAIEmbeddings\n",
        "\n",
        "# # OpenAI Embedding\n",
        "# embedding_model = AzureOpenAIEmbeddings(\n",
        "#     azure_deployment = \"dp-text-embedding-ada-002\",\n",
        "#     openai_api_version = \"2024-05-01-preview\"\n",
        "# )\n",
        "\n",
        "# # HuggingFace Embedding\n",
        "# # hf_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# # Choose based on your use case\n",
        "# text = \"LangChain is a framework for building AI applications.\"\n",
        "# vector = embedding_model.embed_query(text)\n",
        "# vector\n",
        "# print(len(vector), vector[:5])"
      ],
      "metadata": {
        "id": "1UqKRQ6WLXco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating and Managing Vector Stores"
      ],
      "metadata": {
        "id": "JFCEzdZnp6tD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Vector Store (Local Vector Store)"
      ],
      "metadata": {
        "id": "t7ODybu1Mp1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU faiss-cpu"
      ],
      "metadata": {
        "id": "_Pwb9AaYmng1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Initialize Azure OpenAI Embeddings\n",
        "embedding_model = AzureOpenAIEmbeddings(\n",
        "    azure_deployment = \"dp-text-embedding-ada-002\",\n",
        "    openai_api_version = \"2024-05-01-preview\"\n",
        ")\n",
        "\n",
        "# Create texts and vector store\n",
        "texts = [\"LangChain is great for AI.\", \"Vector databases are powerful.\"]\n",
        "vector_store = FAISS.from_texts(texts, embedding_model)\n",
        "\n",
        "# Save the vector store locally\n",
        "vector_store.save_local(\"/content/faiss_store/\")\n",
        "\n",
        "# Load the vector store\n",
        "loaded_vector_store = FAISS.load_local(\n",
        "    \"/content/faiss_store/\",\n",
        "    embedding_model,\n",
        "    allow_dangerous_deserialization=True\n",
        ")"
      ],
      "metadata": {
        "id": "7Hg8cnAel60S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Managing Vector Stores"
      ],
      "metadata": {
        "id": "TPjwkLALNH4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a. Adding New Data\n",
        "new_texts = [\"LangChain supports agents.\", \"FAISS is lightweight.\"]\n",
        "vector_store.add_texts(new_texts)\n",
        "\n",
        "# b. Searching for Similar Data\n",
        "query = \"What is LangChain?\"\n",
        "results = vector_store.similarity_search(query, k=2)  # Retrieve top 2 results\n",
        "for result in results:\n",
        "    print(result.page_content)"
      ],
      "metadata": {
        "id": "qsPQ_5RxmRFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving and Loading Vector Stores\n",
        "# For persistence, save the vector store and reload it when needed.\n",
        "\n",
        "vector_store.save_local(\"faiss_store\")\n",
        "vector_store = loaded_vector_store = FAISS.load_local(\n",
        "    \"faiss_store\",\n",
        "    embedding_model,\n",
        "    allow_dangerous_deserialization=True\n",
        ")"
      ],
      "metadata": {
        "id": "OUepKS0HmkI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is LangChain?\"\n",
        "results = loaded_vector_store.similarity_search(query, k=2)  # Get top 2 similar texts\n",
        "\n",
        "for result in results:\n",
        "    print(result.page_content)\n"
      ],
      "metadata": {
        "id": "I6jPFQiuncLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Pinecone (Cloud-based Vector Store)"
      ],
      "metadata": {
        "id": "FMZZ5uuCqjDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai langchain-pinecone"
      ],
      "metadata": {
        "id": "8Qpo2ZFSqlz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from pinecone import Pinecone\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"<insert key here>\"\n",
        "\n",
        "# Create Azure OpenAI embeddings\n",
        "embeddings = AzureOpenAIEmbeddings(\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    azure_deployment=\"dp-text-embedding-ada-002\",\n",
        "    openai_api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
        ")\n",
        "\n",
        "# Create Pinecone vector store\n",
        "vector_store = PineconeVectorStore.from_texts(\n",
        "    texts=[\"LangChain simplifies AI workflows.\", \"Pinecone manages vector stores.\"],\n",
        "    embedding=embeddings,\n",
        "    index_name=\"langchain-demo\"\n",
        ")\n",
        "\n",
        "# Perform similarity search\n",
        "query = \"What is LangChain?\"\n",
        "results = vector_store.similarity_search(query, k=1)\n",
        "\n",
        "# Print results\n",
        "for result in results:\n",
        "    print(result.page_content)\n"
      ],
      "metadata": {
        "id": "g7FFJc_VK8gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Semantic Search and metadata filtering\n"
      ],
      "metadata": {
        "id": "ZiVHS_H3Ob7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering Results\n",
        "texts_with_metadata = [\n",
        "    {\"text\": \"LangChain simplifies AI workflows.\", \"category\": \"AI\"},\n",
        "    {\"text\": \"Semantic search improves search relevance.\", \"category\": \"Search\"},\n",
        "]\n",
        "\n",
        "# Adding metadata to the vector store\n",
        "vector_store_with_metadata = FAISS.from_texts(\n",
        "    [item[\"text\"] for item in texts_with_metadata],\n",
        "    embedding_model,\n",
        "    metadatas=[{\"category\": item[\"category\"]} for item in texts_with_metadata]\n",
        ")\n",
        "\n",
        "# Search with a filter\n",
        "query = \"How does search work?\"\n",
        "results = vector_store_with_metadata.similarity_search(\n",
        "    query, k=1, filter={\"category\": \"Search\"}\n",
        ")\n",
        "\n",
        "for result in results:\n",
        "    print(result.page_content)\n",
        "# example results\n",
        "# Semantic search improves search relevance."
      ],
      "metadata": {
        "id": "5La2OCis2pWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to agents in LangChain"
      ],
      "metadata": {
        "id": "dWycQ7bCbYfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of agents (e.g., zero-shot, conversational)\n"
      ],
      "metadata": {
        "id": "pvd6IueLkDI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### zero-shot"
      ],
      "metadata": {
        "id": "cRbz_SpzQW2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.tools import tool\n",
        "from langchain.agents import initialize_agent, Tool, AgentType\n",
        "from langchain.llms import OpenAI\n",
        "from langchain_openai import AzureOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.agents import Tool, AgentExecutor\n",
        "\n",
        "# Define the calculator tool\n",
        "@tool\n",
        "def calculator_tool(expression: str) -> str:\n",
        "    \"\"\"A simple calculator for evaluating expressions.\"\"\"\n",
        "    try:\n",
        "        result = eval(expression)\n",
        "        return str(result)\n",
        "    except Exception:\n",
        "        return \"Invalid input.\"\n",
        "\n",
        "# Define the tools\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Calculator\",\n",
        "        func=calculator_tool,\n",
        "        description=\"Use this tool to evaluate mathematical expressions.\",\n",
        "    )\n",
        "]\n",
        "\n",
        "# Define the Azure OpenAI LLM\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "\n",
        "# Define the prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"query\"],\n",
        "    template=\"\"\"Answer the following question: {input}\n",
        "{query}\"\"\",\n",
        ")\n",
        "\n",
        "# Initialize the agent\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Define a query\n",
        "query = \"What is 12 * 8 plus 100?\"\n",
        "\n",
        "# Execute the query\n",
        "response = agent.run(input=query)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "SMySJFCfbY6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversational Agent"
      ],
      "metadata": {
        "id": "VYk38Vn7ktyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.agents import create_openai_functions_agent\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Initialize the LLM (OpenAI GPT)\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "\n",
        "# Configure Memory for Conversation History\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# Initialize the Conversational Agent\n",
        "agent_chain = initialize_agent(\n",
        "    tools=[],  # Add tools here if needed (e.g., calculators, search tools)\n",
        "    llm=llm,\n",
        "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    memory=memory,\n",
        "    verbose=True  # Set to True for detailed logs of interactions.\n",
        ")\n",
        "\n",
        "# Interact with the Agent\n",
        "print(\"Welcome to the Conversational Agent! Type 'exit' to quit.\\n\")\n",
        "\n",
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Exit condition\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Run the agent and get a response\n",
        "    response = agent_chain.run(input=user_input)\n",
        "\n",
        "    # Print the response from the agent\n",
        "    print(f\"Agent: {response}\")\n"
      ],
      "metadata": {
        "id": "ILcMpfdAeAob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools-Enabled Agent"
      ],
      "metadata": {
        "id": "kr8twv5kkp9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities import SQLDatabase\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.agent_toolkits import create_sql_agent\n",
        "\n",
        "# Initialize the database\n",
        "db = SQLDatabase.from_uri(\"sqlite:///Langchain.db\")\n",
        "\n",
        "# Initialize the language model\n",
        "llm = llm\n",
        "\n",
        "# Create the SQL agent\n",
        "agent_executor = create_sql_agent(llm, db=db, verbose=True)\n",
        "\n",
        "# Run a query\n",
        "resp = agent_executor.run(\"Show me the first 5 rows of the 'Sample' table.\")\n",
        "print(resp)\n"
      ],
      "metadata": {
        "id": "lLZRSmaFeEkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Agents"
      ],
      "metadata": {
        "id": "y5K2xor1TF-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, AgentType\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.tools import StructuredTool\n",
        "from typing import Optional\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import SystemMessage\n",
        "\n",
        "# Define the currency conversion tool\n",
        "def currency_conversion_tool(amount: float, from_currency: str, to_currency: str) -> Optional[float]:\n",
        "    \"\"\"Convert between USD, EUR and INR currencies\"\"\"\n",
        "    rates = {\n",
        "    \"USD-INR\": 83.0,\n",
        "    \"EUR-USD\": 1.1,\n",
        "    \"INR-USD\": 0.012,\n",
        "    \"USD-EUR\": 0.9\n",
        "}\n",
        "    pair = f\"{from_currency}-{to_currency}\"\n",
        "    if pair in rates:\n",
        "        return amount * rates[pair]\n",
        "    elif f\"{to_currency}-{from_currency}\" in rates:\n",
        "        return amount / rates[f\"{to_currency}-{from_currency}\"]\n",
        "    return None\n",
        "\n",
        "# Create a LangChain tool with better description\n",
        "currency_tool = StructuredTool.from_function(\n",
        "    func=currency_conversion_tool,\n",
        "    name=\"currency_converter\",\n",
        "    description=\"\"\"Convert amounts between currencies (USD, EUR, INR).\n",
        "    Args:\n",
        "        amount (float): The amount to convert\n",
        "        from_currency (str): Source currency code (USD, EUR, or INR)\n",
        "        to_currency (str): Target currency code (USD, EUR, or INR)\n",
        "    Returns:\n",
        "        float: The converted amount\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "\n",
        "# Create system message for the agent\n",
        "system_message = \"\"\"You are a helpful currency conversion assistant.\n",
        "When given a currency conversion request:\n",
        "1. Extract the amount, source currency, and target currency\n",
        "2. Use the currency_converter tool to perform the conversion\n",
        "3. Always respond with the converted amount in a clear format\n",
        "4. If there's an error, explain what went wrong\n",
        "\n",
        "Format your response as:\n",
        "{amount} {from_currency} = {converted_amount} {to_currency}\"\"\"\n",
        "\n",
        "# Create the agent with custom configuration\n",
        "agent = initialize_agent(\n",
        "    tools=[currency_tool],\n",
        "    llm=llm,\n",
        "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    system_message=system_message,\n",
        "    handle_parsing_errors=True\n",
        ")\n",
        "\n",
        "# Example usage with error handling\n",
        "def convert_currency(query: str) -> str:\n",
        "    try:\n",
        "        result = agent.run(query)\n",
        "        return result if result else \"Sorry, couldn't perform the conversion. Please check the currency codes.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error performing conversion: {str(e)}\"\n",
        "\n",
        "# Test the conversion\n",
        "query = \"Convert 100 EUR to USD\"\n",
        "print(convert_currency(query))"
      ],
      "metadata": {
        "id": "zfyl8oQIeIvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Prompts"
      ],
      "metadata": {
        "id": "Do5b_DtRTSTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XrvyoUSzTTWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "custom_prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"input\"],\n",
        "    template=(\n",
        "        \"You are a helpful assistant specializing in customer support.\\n\"\n",
        "        \"Conversation History:\\n{history}\\n\"\n",
        "        \"User Query: {input}\\n\"\n",
        "        \"Provide a detailed and polite response.\"\n",
        "    ),\n",
        ")\n",
        "agent = create_structured_chat_agent(llm=llm, tools=tools, prompt=custom_prompt, memory=memory)"
      ],
      "metadata": {
        "id": "G9mqu88gTTl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Models and LLMs"
      ],
      "metadata": {
        "id": "FebFx3zZtnWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuring and fine-tuning chat models"
      ],
      "metadata": {
        "id": "UqboQhSd7rZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Incorporating Memory for Multi-Turn Conversations"
      ],
      "metadata": {
        "id": "RVkTNsDiOYOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(llm=llm, memory=memory)\n",
        "\n",
        "conversation.predict(input=\"Hello, can you help me plan my day?\")\n",
        "conversation.predict(input=\"Where did I go yesterday?\")"
      ],
      "metadata": {
        "id": "UR0LfIZkpRMM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4db17bec-6656-4033-aec7-8aff63f688ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' According to your GPS data, you went to work in the morning, then to the gym in the evening, and finally to a restaurant for dinner. Is there anything else you would like to know about your activities yesterday?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "# Create a chat template with system and human messages\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(content='You respond only in the JSON format.'),\n",
        "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Fill in the specific values for n and area\n",
        "messages = chat_template.format_messages(n='5', area='Asia')\n",
        "print(messages)  # Outputs the formatted chat messages\n",
        "\n",
        "output = llm.invoke(messages)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "k8xYGWN8qhGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d7687c-e4df-4ae0-a162-5eaa05afc8d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='You respond only in the JSON format.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Top 5 countries in Asia by population.', additional_kwargs={}, response_metadata={})]\n",
            "\n",
            "AI System: {\"Countries\": [{\"Country\": \"China\", \"Population\": \"1,439,323,776\"}, {\"Country\": \"India\", \"Population\": \"1,380,004,385\"}, {\"Country\": \"Indonesia\", \"Population\": \"273,523,615\"}, {\"Country\": \"Pakistan\", \"Population\": \"220,892,340\"}, {\"Country\": \"Bangladesh\", \"Population\": \"164,689,383\"}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Expression Language (LCEL)"
      ],
      "metadata": {
        "id": "jrJ8YpPtuT6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1: Basic LCEL Syntax"
      ],
      "metadata": {
        "id": "yvUC7kPq5Q2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
        "model = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "chain = prompt | model\n",
        "chain.invoke({\"topic\": \"bears\"})\n"
      ],
      "metadata": {
        "id": "V-jBvNmdXfmz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "cbcefc9f-4a2d-4632-c98c-f7868139b0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nAI: Why did the bear go to the doctor?\\n\\nBecause he was feeling grizzly!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2: LCEL allows for the creation of more sophisticated chains."
      ],
      "metadata": {
        "id": "h0Gb1YAG5OAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Example :LCEL allows for the creation of more sophisticated chains. Here's an example that includes multiple steps:\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
        "model = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "output_parser = StrOutputParser()\n",
        "chain = prompt | model | output_parser\n",
        "chain.invoke({\"topic\": \"bears\"})\n"
      ],
      "metadata": {
        "id": "Z_uiVFmbXjQk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "0a78256f-682e-4a11-b436-fd41a618fff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nRobot: Why did the bear wear a tuxedo?\\nBecause he wanted to look \"bearly\" dressed!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 3: Using RunnableParallel for Multiple Inputs LCEL supports parallel operations.\n"
      ],
      "metadata": {
        "id": "dlxxA7kC5LEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
        "from langchain_openai import AzureOpenAI\n",
        "\n",
        "# Define the prompt template for summarization and translation\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"Summarize the following text: {text}\n",
        "    Then translate the summary to {language}.\"\"\"  # Added newline for better formatting\n",
        ")\n",
        "\n",
        "# Initialize the model\n",
        "model = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "\n",
        "# Define the RunnableParallel operation (modified)\n",
        "summarize_and_translate = RunnableParallel(\n",
        "    summary=prompt | model,\n",
        "    # original_text=RunnablePassthrough()  # Removed as it's not used in the chain\n",
        ")\n",
        "\n",
        "# Define the complete chain (modified)\n",
        "chain = summarize_and_translate  # Removed extra prompt and model invocation\n",
        "\n",
        "# Invoke the chain with input values\n",
        "result = chain.invoke({\n",
        "    \"text\": \"LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis\",\n",
        "    \"language\": \"French\"\n",
        "})\n",
        "\n",
        "# Print the result (optional)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "_kGbQyzaXnHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b7417f-d49c-4bb4-e283-33329d192462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'summary': \"\\n\\nLangChain est un cadre logiciel qui facilite l'intgration de grands modles de langage (LLM) dans les applications. En tant que cadre d'intgration de modle de langage, les cas d'utilisation de LangChain chevauchent largement ceux des modles de langage en gnral, y compris l'analyse et la synthse de documents, les chatbots et l'analyse de code.\\n\\nLangChain est un cadre logiciel pour intgrer les grands modles de langage dans les applications. Les cas d'utilisation de LangChain sont similaires  ceux des modles de langage, tels que l'analyse et la synthse de documents, les chatbots et l'analyse de code.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 4: Error Handling in LCEL"
      ],
      "metadata": {
        "id": "kfabJTdr5Hdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "from langchain.callbacks.manager import CallbackManagerForChainRun\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "from langchain_openai import AzureOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
        "model = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "\n",
        "def fake_error_handler(error: Exception, run_manager: Optional[CallbackManagerForChainRun]) -> str:\n",
        "    return \"Oops! Something went wrong.\"\n",
        "\n",
        "# Wrap the error handler in a RunnableLambda\n",
        "runnable_error_handler = RunnableLambda(lambda x: fake_error_handler(x, None)) # type: ignore\n",
        "\n",
        "# Modified chain with the runnable error handler\n",
        "chain = prompt | model.with_fallbacks([runnable_error_handler])\n",
        "\n",
        "# Invoke the chain to generate a joke about bears\n",
        "result = chain.invoke({\"topic\": \"bears\"})\n",
        "print(result) # Print or use the result as needed"
      ],
      "metadata": {
        "id": "topTiWMsXqUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f586c851-78a6-4238-eb73-c767aaea585f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "AI: Why was the bear so good at math? Because he was a natural at polar-izing numbers!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 5: Using LCEL with Retriever"
      ],
      "metadata": {
        "id": "dp8W_i2JSc6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Initialize Azure OpenAI Embeddings\n",
        "embedding_model = AzureOpenAIEmbeddings(\n",
        "    azure_deployment = \"dp-text-embedding-ada-002\",\n",
        "    openai_api_version = \"2024-05-01-preview\"\n",
        ")\n",
        "\n",
        "# Create texts and vector store\n",
        "texts = [\"LangChain is great for AI.\", \"Vector databases are powerful.\"]\n",
        "vector_store = FAISS.from_texts(texts, embedding_model)\n",
        "\n",
        "# Save the vector store locally\n",
        "vector_store.save_local(\"/content/faiss_store/\")\n",
        "\n",
        "# Load the vector store\n",
        "loaded_vector_store = FAISS.load_local(\n",
        "    \"/content/faiss_store/\",\n",
        "    embedding_model,\n",
        "    allow_dangerous_deserialization=True\n",
        ")"
      ],
      "metadata": {
        "id": "H_SMsNxeS1hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "## Load existing Vector DB FAISS, that we created in the section \"Create a Vector Store (Local Vector Store)\"\n",
        "vectorstore = FAISS.load_local(\n",
        "                              \"/content/faiss_store/\",\n",
        "                              embedding_model,\n",
        "                              allow_dangerous_deserialization=True\n",
        "                          )\n",
        "\n",
        "# Adding New Data\n",
        "new_texts = [\"harrison worked at google\", \"harrison likes spicy food\"]\n",
        "vectorstore.add_texts(new_texts)\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "            {context}\n",
        "\n",
        "            Question: {question}\n",
        "            \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = llm\n",
        "\n",
        "chain = (\n",
        "    RunnableParallel(\n",
        "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    )\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke(\"where did harrison work?\")"
      ],
      "metadata": {
        "id": "FJ6AaXewXvhe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a1e5eeba-d732-4e31-80b8-78c618dbf2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nGoogle.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 6: LCEL advanced Chain"
      ],
      "metadata": {
        "id": "MpjpFnvjTtrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "# Define the prompt template\n",
        "prompt_template = ChatPromptTemplate.from_template(\n",
        "    \"Answer the question based on the context: {context_a} {context_b} Question: {question}\"\n",
        ")\n",
        "\n",
        "# Initialize the model and output parser\n",
        "model = model = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Build the chain\n",
        "chain = (\n",
        "    {\n",
        "        \"context_a\": lambda x: x[\"retriever_a\"],\n",
        "        \"context_b\": lambda x: x[\"retriever_b\"],\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | prompt_template\n",
        "    | model\n",
        "    | output_parser\n",
        ")\n",
        "\n",
        "# Example inputs\n",
        "inputs = {\n",
        "    \"question\": \"What are the benefits of using LangChain?\",\n",
        "    \"retriever_a\": \"LangChain enables modular workflows and easy integration with APIs.\",\n",
        "    \"retriever_b\": \"LangChain supports complex applications like question answering and chatbots.\"\n",
        "}\n",
        "\n",
        "# Execute the chain\n",
        "result = chain.invoke(inputs)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "AJbk92kCuV3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07cc677d-ebe2-4d5d-b528-230389f3ad0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "The benefits of using LangChain include enabling modular workflows and easy integration with APIs, as well as supporting complex applications like question answering and chatbots.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End of Chapter 3"
      ],
      "metadata": {
        "id": "BhhmU-qy367T"
      }
    }
  ]
}
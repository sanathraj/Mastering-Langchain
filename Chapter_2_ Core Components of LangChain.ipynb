{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFrL6CYb_nog"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28Dg53L3-FN1",
        "outputId": "bc9eb437-f322-4bbe-9e65-fe35bcc84416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Library langchain version: 0.3.15\n",
            "Library langchain_community not found. Installing...\n",
            "Library huggingface_hub version: 0.27.1\n",
            "Library langchain_openai not found. Installing...\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "libraries = [\"langchain\", \"langchain_community\", \"huggingface_hub\", \"langchain_openai\"]\n",
        "\n",
        "for library in libraries:\n",
        "    try:\n",
        "        # Try to import the library\n",
        "        module = importlib.import_module(library)\n",
        "        print(f\"Library {library} version: {module.__version__}\")\n",
        "    except ImportError:\n",
        "        # If library is not installed, attempt to install it\n",
        "        print(f\"Library {library} not found. Installing...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", library])\n",
        "        # After installing, import again and print the version\n",
        "        module = importlib.import_module(library)\n",
        "        # print(f\"Library {library} version after installation: {module.__version__}\")\n",
        "    except AttributeError:\n",
        "        # If library doesn't have __version__ attribute\n",
        "        print(f\"Library {library} does not have a __version__ attribute.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U duckduckgo-search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZY636xTRCaY",
        "outputId": "5406cf3b-c122-4473-cd5f-bcafaf8b4b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: duckduckgo-search in /usr/local/lib/python3.11/dist-packages (7.2.1)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (8.1.8)\n",
            "Requirement already satisfied: primp>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (0.10.1)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (5.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u6TKBn32neW",
        "outputId": "d12e5a9f-ef12-460a-c754-cc6f7758091e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.11\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcdqBAic-t0s"
      },
      "outputs": [],
      "source": [
        "# from google.colab import userdata\n",
        "# HF_TOKEN = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8OPmHwl5TNW"
      },
      "source": [
        "##Azure Open AI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT6UPs0h5b4I"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import AzureOpenAI\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2024-05-01-preview\"\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get('AZ_OPENAI_KEY')\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] =  \"https://azopenai-demo.openai.azure.com/\"\n",
        "\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "embeding_small = AzureOpenAI(deployment_name=\"dp-text-embedding-ada-002\", model_name=\"text-embedding-ada-002\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk8W1LhTLUu_",
        "outputId": "981ea7b9-082f-4e0c-9454-4f4f67e19b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AzureOpenAI(client=<openai.resources.completions.Completions object at 0x7810dd5f3190>, async_client=<openai.resources.completions.AsyncCompletions object at 0x7810dd5da290>, model_name='text-embedding-3-small', model_kwargs={}, openai_api_key=SecretStr('**********'), azure_endpoint='https://azopenai-demo.openai.azure.com/', deployment_name='dp-text-embedding-3-small', openai_api_version='2024-05-01-preview', openai_api_type='azure')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YhaW5CTBWWe"
      },
      "source": [
        "# Chapter 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chains"
      ],
      "metadata": {
        "id": "I3aiB5JuQAvu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBvJGegABZEN"
      },
      "source": [
        "## LLMChain: Basic sequential processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66ZsAdAiC2Xs",
        "outputId": "38a3cc18-cabe-4323-e089-9538d0068f5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output:\n",
            " {'food': 'pizza', 'language': 'English', 'text': '\\n\\n- Pizza dough\\n- Pizza sauce\\n- Shredded mozzarella cheese\\n- Toppings of your choice (e.g. pepperoni, mushrooms, bell peppers)\\n- Olive oil\\n- Garlic powder\\n- Italian seasoning\\n- Salt\\n- Black pepper'}\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "template = '''You are an experience cook.\n",
        "list the ingredients to cook the following fooditem \"{food}\" in {language}.'''\n",
        "prompt_template = PromptTemplate.from_template(template=template)\n",
        "\n",
        "# Initialize the language model\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt_template,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "output = chain.invoke({'food': 'pizza', 'language': 'English'})\n",
        "print(\"output:\\n\", output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "688I2ngID3y-"
      },
      "source": [
        "## Sequential Chain: Combining multiple chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emmfPMX2B5fy",
        "outputId": "4ebcb53d-878b-4e97-afe0-8523f9b44582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "1. To improve battery life, the product could be equipped with a larger battery capacity or optimized software to reduce battery drain. Additionally, the inclusion of fast charging technology could also help alleviate battery life concerns.\n",
            "\n",
            "2. To address dissatisfaction with camera quality, the product could be equipped with a higher resolution camera, improved image processing software, or additional camera features such as optical image stabilization or a dual-lens setup.\n",
            "\n",
            "3. To meet the expectation for better photo quality, the product could undergo rigorous testing and fine-tuning of the camera's hardware and software. This could include collaborating with professional photographers to ensure the best possible photo quality.\n",
            "\n",
            "4. To extend battery life, the product could have an energy-saving mode or the option to disable certain features that may drain battery life. Additionally, the product could have a larger battery capacity or improved software optimization to increase overall battery life.\n",
            "\n",
            "5. To address disappointment with high-performance app usage, the product could be equipped with a more powerful processor and increased RAM to handle demanding apps and tasks more efficiently. The software could also be optimized to better manage resources and improve overall performance.\n",
            "\n",
            "6. To continue the appreciation for build quality and design, the product could maintain its high-quality materials and sleek design while also incorporating new features and improvements.\n",
            "\n",
            "7.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import SequentialChain, LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import AzureOpenAI\n",
        "\n",
        "# Initialize the LLM (Azure OpenAI in this case)\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "\n",
        "# Step 1: Summarize the customer feedback\n",
        "template1 = \"Summarize the following customer feedback:\\n{feedback}\"\n",
        "prompt1 = ChatPromptTemplate.from_template(template1)\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"feedback_summary\")\n",
        "\n",
        "# Step 2: Identify key issues from the feedback summary\n",
        "template2 = \"Identify the key issues or complaints from this feedback summary:\\n{feedback_summary}\"\n",
        "prompt2 = ChatPromptTemplate.from_template(template2)\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"key_issues\")\n",
        "\n",
        "# Step 3: Generate improvement suggestions based on the key issues\n",
        "template3 = \"Based on these key issues, suggest improvements to the product:\\n{key_issues}\"\n",
        "prompt3 = ChatPromptTemplate.from_template(template3)\n",
        "chain_3 = LLMChain(llm=llm, prompt=prompt3, output_key=\"improvement_suggestions\")\n",
        "\n",
        "# Create the Sequential Chain\n",
        "seq_chain = SequentialChain(chains=[chain_1, chain_2, chain_3], input_variables=['feedback'], output_variables=['feedback_summary', 'key_issues', 'improvement_suggestions'], verbose=True)\n",
        "\n",
        "# Example Customer Feedback\n",
        "customer_feedback = '''\n",
        "I have been using this smartphone for about 6 months now, and while it performs well in terms of speed,\n",
        "I am facing significant issues with battery life. The battery drains very quickly, especially when using apps with high performance requirements.\n",
        "Also, the camera quality does not match what was advertised. I expected better photo quality, particularly in low-light conditions.\n",
        "On the positive side, I appreciate the build quality and the design, but the battery and camera need major improvements.\n",
        "'''\n",
        "\n",
        "# Run the Sequential Chain\n",
        "results = seq_chain.invoke(customer_feedback)\n",
        "\n",
        "# Print the final improvement suggestions\n",
        "print(results['improvement_suggestions'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RouterChain: Dynamic routing based on input"
      ],
      "metadata": {
        "id": "siH1WgfTKWxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "beginner_template = '''You are a physics teacher who is really\n",
        "focused on beginners and explaining complex topics in simple to understand terms.\n",
        "You assume no prior knowledge. Here is the question\\n{input}'''\n",
        "\n",
        "expert_template = '''You are a world expert physics professor who explains physics topics\n",
        "to advanced audience members. You can assume anyone you answer has a\n",
        "PhD level understanding of Physics. Here is the question\\n{input}'''\n",
        "\n",
        "prompt_infos = [\n",
        "\n",
        "    {'name':'advanced physics','description': 'Answers advanced physics questions',\n",
        "     'prompt_template':expert_template},\n",
        "    {'name':'beginner physics','description': 'Answers basic beginner physics questions',\n",
        "     'prompt_template':beginner_template},\n",
        "\n",
        "]\n",
        "\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm,prompt=default_prompt)\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
        "\n",
        "\n",
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )\n",
        "\n",
        "chain.run(\"How do magnets work?\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "FONzxrcoqJl3",
        "outputId": "9d7acaee-f906-427c-e1ee-e4a2f9893931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-651fcbe09af7>:52: LangChainDeprecationWarning: Please see migration guide here for recommended implementation: https://python.langchain.com/docs/versions/migrating_chains/multi_prompt_chain/\n",
            "  chain = MultiPromptChain(router_chain=router_chain,\n",
            "<ipython-input-9-651fcbe09af7>:57: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  chain.run(\"How do magnets work?\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "beginner physics: {'input': 'How do magnets work?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nMagnets work by creating a magnetic field around them. This magnetic field is made up of invisible lines of force that extend from one end of the magnet to the other. When a magnet is near another object, such as another magnet or a piece of metal, these lines of force interact with the electrons in the object. This interaction causes the object to either be attracted to or repelled by the magnet, depending on the orientation of the object's electrons. This is why magnets can either stick to or push away from each other. Additionally, magnets have two poles, a north pole and a south pole, and opposite poles attract each other while like poles repel each other. This is because the lines of force from each pole are trying to connect and cancel each other out. So, to sum it up, magnets work by creating a magnetic field that interacts with the electrons in other objects, causing them to either be attracted to or repelled by the magnet.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Routing by Semantic Similarity"
      ],
      "metadata": {
        "id": "vChh_lt9Nujo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.utils.math import cosine_similarity\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "\n",
        "# Example templates for routing by similarity\n",
        "physics_template = \"\"\"You are a smart physics professor, great at answering questions about physics concisely.\n",
        "When you don't know the answer, admit that you don't know.\"\"\"\n",
        "\n",
        "math_template = \"\"\"You are a mathematician. You are great at breaking down complex problems and explaining step-by-step.\"\"\"\n",
        "\n",
        "# Embed the templates for comparison\n",
        "embeddings = AzureOpenAIEmbeddings(model=\"text-embedding-ada-002\", azure_deployment=\"dp-text-embedding-ada-002\")\n",
        "prompt_templates = [physics_template, math_template]\n",
        "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
        "\n",
        "# Function to route based on similarity\n",
        "def prompt_router(input):\n",
        "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
        "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
        "    most_similar = prompt_templates[similarity.argmax()]\n",
        "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
        "    return PromptTemplate.from_template(most_similar)\n",
        "\n",
        "# Creating the chain\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "chain = (\n",
        "    {\"query\": RunnablePassthrough()}\n",
        "    | RunnableLambda(prompt_router)\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Test the chain\n",
        "print(chain.invoke(\"What's a black hole\"))\n"
      ],
      "metadata": {
        "id": "kou-K9A-qS5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0b4620-4b67-478d-d948-2ee4d3630d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using PHYSICS\n",
            "\n",
            "As a physics professor, it is my responsibility to provide accurate and concise answers to questions about physics. While I strive to have a deep understanding of the subject, there may be times when I do not know the answer to a question. In such cases, I will not hesitate to admit that I do not know.\n",
            "\n",
            "Admitting that I do not know the answer does not diminish my expertise or credibility as a professor. In fact, it is a sign of honesty and integrity. It also shows that I am open to learning and expanding my knowledge, just like my students.\n",
            "\n",
            "I believe that it is important to acknowledge when I do not have the answer, as it allows for further exploration and discussion. It also gives me the opportunity to research and find the answer, which can be a valuable learning experience for both myself and my students.\n",
            "\n",
            "In the field of physics, there is always more to learn and discover. As a professor, I am constantly learning and growing alongside my students. So, if I do not know the answer to a question, I will admit it and work towards finding the answer together with my students.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TransformChain: Data transformation and preprocessing"
      ],
      "metadata": {
        "id": "D82_JJoewCZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import TransformChain\n",
        "\n",
        "def transform_func(inputs):\n",
        "    text = inputs[\"text\"]\n",
        "    transformed_text = text.lower()  # Simple lowercase transformation\n",
        "    word_count = len(text.split())\n",
        "    return {\"lowercase_text\": transformed_text, \"word_count\": word_count}\n",
        "\n",
        "transform_chain = TransformChain(\n",
        "    input_variables=[\"text\"],\n",
        "    output_variables=[\"lowercase_text\", \"word_count\"],\n",
        "    transform=transform_func\n",
        ")\n",
        "\n",
        "result = transform_chain.invoke(\"This is an EXAMPLE sentence.\")\n",
        "print(result)\n",
        "# Output: {'lowercase_text': 'this is an example sentence.', 'word_count': 5}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yjhVMQgt4x5",
        "outputId": "10627c51-03ef-41bf-9cc4-b886aacbf6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'This is an EXAMPLE sentence.', 'lowercase_text': 'this is an example sentence.', 'word_count': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MapReduceChain: Parallel processing for large datasets\n"
      ],
      "metadata": {
        "id": "rq_zVVRqwdJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# Initialize the language model\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "\n",
        "# Initialize text splitter\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "\n",
        "# Define map and reduce prompts\n",
        "map_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Summarize this text in one sentence: {text}\"\n",
        ")\n",
        "\n",
        "reduce_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Combine these summaries into a coherent paragraph: {text}\"\n",
        ")\n",
        "\n",
        "# Create the map and reduce chains\n",
        "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
        "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "# Create combine documents chain\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain,\n",
        "    document_variable_name=\"text\"\n",
        ")\n",
        "\n",
        "# Create the map reduce chain\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    llm_chain=map_chain,\n",
        "    # Use reduce_documents_chain instead of combine_documents_chain\n",
        "    reduce_documents_chain=combine_documents_chain,\n",
        "    document_variable_name=\"text\",\n",
        "    return_intermediate_steps=True\n",
        ")\n",
        "\n",
        "\n",
        "# Your text data\n",
        "large_text = '''Before you can begin to determine what the composition of a particular paragraph will be, you must first decide on an argument and a working thesis statement for your paper. What is the most important idea that you are trying to convey to your reader? The information in each paragraph must be related to that idea. In other words, your paragraphs should remind your reader that there is a recurrent relationship between your thesis and the information in each paragraph. A working thesis functions like a seed from which your paper, and your ideas, will grow. The whole process is an organic one—a natural progression from a seed to a full-blown paper where there are direct, familial relationships between all of the ideas in the paper.\n",
        "\n",
        "The decision about what to put into your paragraphs begins with the germination of a seed of ideas; this “germination process” is better known as brainstorming. There are many techniques for brainstorming; whichever one you choose, this stage of paragraph development cannot be skipped. Building paragraphs can be like building a skyscraper: there must be a well-planned foundation that supports what you are building. Any cracks, inconsistencies, or other corruptions of the foundation can cause your whole paper to crumble.\n",
        "\n",
        "So, let’s suppose that you have done some brainstorming to develop your thesis. What else should you keep in mind as you begin to create paragraphs? Every paragraph in a paper should be:\n",
        "\n",
        "Unified: All of the sentences in a single paragraph should be related to a single controlling idea (often expressed in the topic sentence of the paragraph).\n",
        "Clearly related to the thesis: The sentences should all refer to the central idea, or thesis, of the paper (Rosen and Behrens 119).\n",
        "Coherent: The sentences should be arranged in a logical manner and should follow a definite plan for development (Rosen and Behrens 119).\n",
        "Well-developed: Every idea discussed in the paragraph should be adequately explained and supported through evidence and details that work together to explain the paragraph’s controlling idea (Rosen and Behrens 119).\n",
        "How do I organize a paragraph?\n",
        "There are many different ways to organize a paragraph. The organization you choose will depend on the controlling idea of the paragraph. Below are a few possibilities for organization, with links to brief examples:\n",
        "\n",
        "Narration: Tell a story. Go chronologically, from start to finish. (See an example.)\n",
        "Description: Provide specific details about what something looks, smells, tastes, sounds, or feels like. Organize spatially, in order of appearance, or by topic. (See an example.)\n",
        "Process: Explain how something works, step by step. Perhaps follow a sequence—first, second, third. (See an example.)\n",
        "Classification: Separate into groups or explain the various parts of a topic. (See an example.)\n",
        "Illustration: Give examples and explain how those examples support your point. (See an example in the 5-step process below.)\n",
        "Illustration paragraph: a 5-step example\n",
        "From the list above, let’s choose “illustration” as our rhetorical purpose. We’ll walk through a 5-step process for building a paragraph that illustrates a point in an argument. For each step there is an explanation and example. Our example paragraph will be about human misconceptions of piranhas.\n",
        "\n",
        "Step 1. Decide on a controlling idea and create a topic sentence\n",
        "Paragraph development begins with the formulation of the controlling idea. This idea directs the paragraph’s development. Often, the controlling idea of a paragraph will appear in the form of a topic sentence. In some cases, you may need more than one sentence to express a paragraph’s controlling idea.\n",
        "\n",
        "Controlling idea and topic sentence — Despite the fact that piranhas are relatively harmless, many people continue to believe the pervasive myth that piranhas are dangerous to humans.\n",
        "\n",
        "Step 2. Elaborate on the controlling idea'''  # Your existing text\n",
        "\n",
        "\n",
        "# Split text into documents\n",
        "texts = text_splitter.split_text(large_text)\n",
        "docs = [Document(page_content=t) for t in texts]\n",
        "\n",
        "# Process the documents\n",
        "try:\n",
        "    result = map_reduce_chain.invoke(docs)\n",
        "    print(\"\\nFinal Summary:\")\n",
        "    print(result['output_text'])\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aMiZ96ow4gF",
        "outputId": "bbbc7ce5-b155-409d-c740-3b0fc6ee0e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Summary:\n",
            " Each paragraph should also flow logically from one to the next, building upon the previous one and contributing to the overall argument and thesis statement. Just as a seed needs a strong foundation and support to grow, a paper requires a strong argument and well-structured paragraphs to effectively convey its message. Brainstorming and organizing ideas are crucial steps in this process, as they help to develop a coherent and well-supported central idea. By utilizing different structures such as narration, description, process, classification, and illustration, a writer can effectively present and support their argument, just as each part of a seed supports its growth into a strong and thriving plant.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom chains: Creating specialized chains for specific tasks"
      ],
      "metadata": {
        "id": "1FSXkUk-OsuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain, SequentialChain\n",
        "from langchain.llms import AzureOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "class ArticleGeneratorChain(SequentialChain):\n",
        "    def __init__(self, llm):\n",
        "        # Article generation chain\n",
        "        article_prompt = PromptTemplate(\n",
        "            input_variables=[\"topic\"],\n",
        "            template=\"Write a short article about {topic}.\"\n",
        "        )\n",
        "        article_chain = LLMChain(\n",
        "            llm=llm,\n",
        "            prompt=article_prompt,\n",
        "            output_key=\"article\"\n",
        "        )\n",
        "\n",
        "        # Fact-checking chain\n",
        "        fact_check_prompt = PromptTemplate(\n",
        "            input_variables=[\"article\"],\n",
        "            template=\"Identify any factual claims in this article and rate their accuracy: {article}\"\n",
        "        )\n",
        "        fact_check_chain = LLMChain(\n",
        "            llm=llm,\n",
        "            prompt=fact_check_prompt,\n",
        "            output_key=\"fact_check\"\n",
        "        )\n",
        "\n",
        "        # Revision chain\n",
        "        revision_prompt = PromptTemplate(\n",
        "            input_variables=[\"article\", \"fact_check\"],\n",
        "            template=\"Revise this article based on the fact-check results:\\n\\n Article: {article}\\n\\nFact-check: {fact_check}\\n\\nRevised article:\"\n",
        "        )\n",
        "        revision_chain = LLMChain(\n",
        "            llm=llm,\n",
        "            prompt=revision_prompt,\n",
        "            output_key=\"revised_article\"\n",
        "        )\n",
        "\n",
        "        # Initialize the sequential chain\n",
        "        super().__init__(\n",
        "            chains=[article_chain, fact_check_chain, revision_chain],\n",
        "            input_variables=[\"topic\"],\n",
        "            output_variables=[\"article\", \"fact_check\", \"revised_article\"]\n",
        "        )\n",
        "\n",
        "# Usage\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "article_generator = ArticleGeneratorChain(llm)\n",
        "result = article_generator.invoke({\"topic\": \"Singularity will be achieved by 2025\"})\n",
        "print(\"fact_check results\\n\", result['fact_check'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTbgTmKc0RhO",
        "outputId": "c8be2cea-ae23-4802-c18b-a8379a1dcdc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fact_check results\n",
            "  will eventually surpass human intelligence.\n",
            "\n",
            "1. The concept of Singularity refers to a hypothetical future event in which artificial intelligence will surpass human intelligence.\n",
            "Rating: Mostly accurate. While the concept of Singularity is still hypothetical, it is a widely recognized concept in the scientific and technological community.\n",
            "\n",
            "2. Singularity is also known as the technological singularity.\n",
            "Rating: Accurate.\n",
            "\n",
            "3. Many experts and futurists have predicted that Singularity will be achieved by the year 2025.\n",
            "Rating: Inaccurate. While some experts have made predictions about when Singularity may occur, there is no consensus on a specific timeframe.\n",
            "\n",
            "4. The prediction of Singularity by 2025 is based on the rapid advancements in AI and other emerging technologies.\n",
            "Rating: Mostly accurate. The prediction is based on the current advancements in AI, but it is not solely based on them.\n",
            "\n",
            "5. Moore's Law states that the number of transistors in a microchip will double every 18-24 months.\n",
            "Rating: Mostly accurate. Moore's Law has been a consistent trend in the advancement of computing power, but it is important to note that it is not a law and may not continue indefinitely.\n",
            "\n",
            "6. The growth of computing power is expected to continue, leading to the development of more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"article results\", result['article'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_nRvJdfB8Je",
        "outputId": "5fcd1a7c-98d7-41ed-968d-8c7d737ca99e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "article results \n",
            "\n",
            "The rise of artificial intelligence (AI) has undoubtedly revolutionized the way we live, work, and interact with technology. From virtual assistants and chatbots to self-driving cars and smart homes, AI has become an integral part of our daily lives. However, with this rapid advancement in technology, there are concerns about the impact of AI on job markets.\n",
            "\n",
            "One of the main concerns is that AI will replace human workers, leading to job losses and unemployment. While it is true that some jobs may become obsolete with the implementation of AI, it is also creating new job opportunities. AI requires human input for programming, maintenance, and development, leading to the creation of new roles in the tech industry.\n",
            "\n",
            "Moreover, AI can also enhance productivity and efficiency in many industries, leading to the creation of new job roles. For example, in the healthcare sector, AI can assist in medical diagnosis and drug discovery, leading to better patient care and the need for more healthcare professionals.\n",
            "\n",
            "Another impact of AI on job markets is the need for upskilling and reskilling. With the introduction of AI, certain job skills may become redundant, and workers will need to acquire new skills to stay relevant in the job market. This shift towards a more tech-savvy workforce will also require governments and organizations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"revised_article results\", result['revised_article'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAxJi-_HA26p",
        "outputId": "ff8fa555-3f71-47ae-9ff0-6c9448c8dda6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "revised_article results \n",
            "\n",
            "The rise of artificial intelligence (AI) has undoubtedly brought about significant changes in the way we live, work, and interact with technology. From virtual assistants and chatbots to self-driving cars and smart homes, AI has become an integral part of our daily lives. However, with this rapid advancement in technology, there are concerns about its impact on job markets.\n",
            "\n",
            "One of the main concerns is that AI will replace human workers, leading to job losses and unemployment. While it is true that some jobs may become obsolete with the implementation of AI, the technology is also creating new job opportunities. AI requires human input for programming, maintenance, and development, leading to the creation of new roles in the tech industry.\n",
            "\n",
            "Moreover, AI can also enhance productivity and efficiency in many industries, resulting in the creation of new job roles. For example, in the healthcare sector, AI can assist in medical diagnosis and drug discovery, leading to better patient care and an increased demand for healthcare professionals.\n",
            "\n",
            "Another impact of AI on job markets is the need for upskilling and reskilling. As AI becomes more prevalent, certain job skills may become redundant, and workers will need to acquire new skills to stay relevant in the job market. This shift towards a more tech-savvy workforce will also require governments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizing chain performance\n"
      ],
      "metadata": {
        "id": "4iI5nsruGll5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caching\n",
        "from langchain.cache import InMemoryCache\n",
        "from langchain.globals import set_llm_cache\n",
        "\n",
        "# Set up in-memory cache\n",
        "set_llm_cache(InMemoryCache())\n",
        "\n",
        "# Now any calls to LLMs will be cached"
      ],
      "metadata": {
        "id": "ZRQ1ZHg5BghE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lazy evaluation"
      ],
      "metadata": {
        "id": "xp878H9jPsIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LazyChain:\n",
        "    def __init__(self, llm, prompt_template):\n",
        "        self.llm = llm\n",
        "        self.prompt_template = prompt_template  # Store as prompt_template\n",
        "        self._result = None\n",
        "\n",
        "    def get_result(self, input_text: str) -> str:\n",
        "        if self._result is None:\n",
        "            # Create PromptTemplate instance here\n",
        "            prompt = PromptTemplate.from_template(self.prompt_template)\n",
        "            chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "            self._result = chain.run(input_text)\n",
        "        return self._result\n",
        "\n",
        "# Example usage\n",
        "lazy_chain = LazyChain(llm, \"Analyze: {text}\")\n",
        "result = lazy_chain.get_result(\"How is AI shaping the modern world\")\n",
        "print(\"result:\\n\", result)"
      ],
      "metadata": {
        "id": "X7Ut5p3FDTgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f35257e8-c154-4305-ddf9-d79fc19d5301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result:\n",
            " \n",
            "\n",
            "Artificial intelligence (AI) is quickly shaping the modern world in a variety of ways. From revolutionizing industries to changing the way we live and work, AI has become an integral part of our daily lives. Here are some of the ways in which AI is shaping the modern world:\n",
            "\n",
            "1. Automation and efficiency: One of the key ways in which AI is shaping the modern world is through automation and efficiency. AI-powered machines and systems are able to perform tasks faster, more accurately, and with less human intervention. This has led to increased productivity and cost savings in industries such as manufacturing, healthcare, and finance.\n",
            "\n",
            "2. Personalization: AI is also shaping the modern world by personalizing experiences for individuals. With the help of AI, companies can analyze vast amounts of data to understand customer preferences and behavior, and offer personalized products and services. This has led to improved customer satisfaction and loyalty.\n",
            "\n",
            "3. Improving healthcare: AI is playing a crucial role in improving healthcare by accurately diagnosing diseases, assisting in surgeries, and developing personalized treatment plans. AI-powered medical devices and software are also helping healthcare professionals to make better decisions and provide better care to patients.\n",
            "\n",
            "4. Advancements in transportation: AI is also transforming the way we travel. With the development of self-driving cars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Templates"
      ],
      "metadata": {
        "id": "e_8BaHqfQJG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic prompt generation"
      ],
      "metadata": {
        "id": "5_ABPr4EImfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dynamic_prompt(user_input, context):\n",
        "    if \"summarize\" in user_input.lower():\n",
        "        template = \"Summarize the following text in 3 sentences: {text}\"\n",
        "        return template.format(text=context)\n",
        "    elif \"question\" in user_input.lower():\n",
        "        template = \"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
        "        return template.format(context=context, question=user_input)\n",
        "    else:\n",
        "        return f\"Please respond to the following: {user_input}\"\n"
      ],
      "metadata": {
        "id": "U_aaEPkFGewd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_dynamic_prompt(\"question the imnpact of AI good for mankind\",\"this is 2025\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lou7Drj_Hw5E",
        "outputId": "91407f1d-cbc4-44ed-e7b2-10004ecdffee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Context: this is 2025\\nQuestion: question the imnpact of AI good for mankind\\nAnswer:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt optimization techniques"
      ],
      "metadata": {
        "id": "ZVQPiBf9Kzc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "\n",
        "# Define the example prompt template\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"output\"],\n",
        "    template=\"Input: {input}\\nOutput: {output}\"\n",
        ")\n",
        "\n",
        "# Define the few-shot prompt template\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    examples=[\n",
        "        {\"input\": \"This film was a complete waste of time. The plot was confusing and the acting was terrible?\", \"output\": \"Negative\"},\n",
        "        {\"input\": \"I was blown away by the stunning visuals and compelling storyline. A must-see!\", \"output\": \"Positive\"},\n",
        "    ],\n",
        "    example_prompt=example_prompt,\n",
        "    prefix=\"Answer the following questions:\\n\",\n",
        "    suffix=\"{input}\\nOutput:\",\n",
        "    input_variables=[\"input\"],\n",
        ")\n",
        "\n",
        "# Use the few-shot prompt in your chain\n",
        "chain = LLMChain(llm=llm, prompt=few_shot_prompt)\n",
        "result = chain.invoke({\"input\": \"very bad decision to watch that movie\"})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V6rfQCVH7s7",
        "outputId": "fe29c8c1-ed38-4335-aa93-6a93b421f122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': 'very bad decision to watch that movie', 'text': ' Negative'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\"input\": \"Ok nothing good or bad to tell\"})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CyLd1WrITLs",
        "outputId": "746ba8e4-7aac-4ffd-9cec-d6497718f07c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': 'Ok nothing good or bad to tell', 'text': ' Neutral'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools and Function Calling"
      ],
      "metadata": {
        "id": "xfWm612CSimx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Built-in tools and their functionalities"
      ],
      "metadata": {
        "id": "IQWzqwhdNEld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install -qU duckduckgo-search langchain-community\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "search = DuckDuckGoSearchRun()\n",
        "search.invoke(\"Einstein's first name?\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "G6B_bjrJIyrS",
        "outputId": "a3c4edb5-5e77-4040-81d5-9ebf4d1330a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Albert Einstein, the brilliant physicist and Nobel laureate, revolutionized our understanding of the universe with his theory of relativity and became a symbol of genius that continues to inspire minds worldwide. Albert Einstein is one of the most famous scientists in history. His name transformed our understanding of the universe. When people hear the word \"genius\", Albert Einstein\\'s name often comes to mind. Among his most popular works is the theory of relativity, one of the main foundations of modern physics. He also famously developed the mass-energy equivalence or E=mc 2, which is the most famous equation in the world. Albert Einstein\\'s numerous scientific breakthroughs had changed the way people see the ... There is some evidence from Einstein\\'s writings that he collaborated with his first wife, Mileva Marić. In 13 December 1900, a first article on capillarity signed only under Albert\\'s name was submitted. A brief biography of Albert Einstein (March 14, 1879 - April 18, 1955), the scientist whose theories changed the way we think about the universe.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating custom tools\n"
      ],
      "metadata": {
        "id": "hoYGn-vlNweR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import tool\n",
        "from typing import Union\n",
        "\n",
        "@tool\n",
        "def calculate_square(number: Union[int, float]) -> Union[int, float]:\n",
        "    \"\"\"\n",
        "    Calculate the square of a given number.\n",
        "    Args:\n",
        "        number (Union[int, float]): The number to be squared.\n",
        "    Returns:\n",
        "        Union[int, float]: The square of the input number.\n",
        "    \"\"\"\n",
        "    return number**2  # Fix: calculate the square correctly\n",
        "\n",
        "# Call calculate_square with a dictionary input\n",
        "result = calculate_square.run({\"number\": 10.0})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "282vVPLjLNbi",
        "outputId": "0c75e37d-38c9-4b3d-b5fb-073f6919ff0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom tool class by inheriting from `BaseTool`"
      ],
      "metadata": {
        "id": "DBCa4GVaPDle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import BaseTool\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional, Type\n",
        "\n",
        "class FactorialInput(BaseModel):\n",
        "    number: int = Field(..., description=\"The number to calculate the factorial of\")\n",
        "\n",
        "class FactorialTool(BaseTool):\n",
        "    name: str = \"FactorialCalculator\"\n",
        "    description: str = \"Calculates the factorial of a given non-negative integer\"\n",
        "    args_schema: Optional[Type[BaseModel]] = FactorialInput\n",
        "\n",
        "    def _run(self, number: int) -> int:\n",
        "        if number < 0:\n",
        "            raise ValueError(\"Factorial is only defined for non-negative integers\")\n",
        "        result = 1\n",
        "        for i in range(1, number + 1):\n",
        "            result *= i\n",
        "        return result\n",
        "\n",
        "    async def _arun(self, number: int) -> int:\n",
        "        # For async execution\n",
        "        return self._run(number)\n",
        "\n",
        "tool = FactorialTool()\n",
        "tool.run({\"number\": 5})"
      ],
      "metadata": {
        "id": "Cnz6vTccNa2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4208a429-811e-443b-fdfe-5a3b854c689e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya9nKGJtN2Pu",
        "outputId": "c89dd820-06c7-498d-bd88-8a67d863566a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##      Integrating external APIs as tools"
      ],
      "metadata": {
        "id": "MPF-2suJWz8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### weather API as a LangChain tool"
      ],
      "metadata": {
        "id": "iQyAcPncSKPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import httpx\n",
        "from langchain.tools import BaseTool\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class WeatherTool(BaseTool):\n",
        "    name: str = \"WeatherInfo\"\n",
        "    description: str = \"Fetch current weather information for a specified city\"\n",
        "\n",
        "    def _run(self, city: str) -> str:\n",
        "        \"\"\"Fetch current weather information for a specified city.\"\"\"\n",
        "        try:\n",
        "            url = \"https://api.exampleweather.com/current\"  # Hypothetical URL\n",
        "            params = {\"city\": city, \"apiKey\": \"your_api_key\"}  # Assume API key is required\n",
        "            response = httpx.get(url, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            weather_condition = data[\"weather\"][\"condition\"]\n",
        "            temperature = data[\"weather\"][\"temperature\"]\n",
        "            return f\"Current weather in {city}: {weather_condition}, Temperature: {temperature}°C\"\n",
        "        except httpx.HTTPStatusError as e:\n",
        "            return f\"Error: Unable to fetch weather data. Status code: {e.response.status_code}\"\n",
        "        except KeyError:\n",
        "            return \"Error: Unexpected response format from the weather API\"\n",
        "        except Exception as e:\n",
        "            return f\"An unexpected error occurred: {str(e)}\"\n",
        "\n",
        "    async def _arun(self, city: str) -> str:\n",
        "        \"\"\"Asynchronous version of the weather fetching tool.\"\"\"\n",
        "        try:\n",
        "            async with httpx.AsyncClient() as client:\n",
        "                url = \"https://api.exampleweather.com/current\"\n",
        "                params = {\"city\": city, \"apiKey\": \"your_api_key\"}\n",
        "                response = await client.get(url, params=params)\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "\n",
        "                weather_condition = data[\"weather\"][\"condition\"]\n",
        "                temperature = data[\"weather\"][\"temperature\"]\n",
        "                return f\"Current weather in {city}: {weather_condition}, Temperature: {temperature}°C\"\n",
        "        except httpx.HTTPStatusError as e:\n",
        "            return f\"Error: Unable to fetch weather data. Status code: {e.response.status_code}\"\n",
        "        except KeyError:\n",
        "            return \"Error: Unexpected response format from the weather API\"\n",
        "        except Exception as e:\n",
        "            return f\"An unexpected error occurred: {str(e)}\""
      ],
      "metadata": {
        "id": "NjhQmFADOXA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function calling: Enhancing LLM capabilities\n"
      ],
      "metadata": {
        "id": "QLm5PZ6ZaN_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "#  from langchain.llms import OpenAI\n",
        "\n",
        "def get_weather(location):\n",
        "  return f\"The weather in {location} is Summer.\"\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"WeatherTool\",\n",
        "        func=get_weather,\n",
        "        description=\"Useful for getting weather information for a specific location\"\n",
        "    )\n",
        "]\n",
        "\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "\n",
        "agent.run(\"What's the weather like in London?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "M41uS2dmSRrw",
        "outputId": "23064774-1d7a-4c9e-8fc8-55e3401f1558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use the WeatherTool to get weather information for London\n",
            "Action: WeatherTool\n",
            "Action Input: London\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThe weather in London is Summer.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know that the weather in London is Summer.\n",
            "Final Answer: The weather in London is Summer.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The weather in London is Summer.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Function Calling Techniques\n"
      ],
      "metadata": {
        "id": "gv9WGox3cNA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chained Function Calls"
      ],
      "metadata": {
        "id": "GqrT_BeXVPCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Given the weather: {weather}, suggest an appropriate outfit.\"\n",
        ")\n",
        "\n",
        "weather_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "outfit_chain = LLMChain(llm=llm, prompt=ChatPromptTemplate.from_template(\n",
        "    \"Describe the outfit: {outfit}\"\n",
        "))\n",
        "\n",
        "def get_outfit_recommendation(location):\n",
        "    weather = get_weather(location)\n",
        "    outfit = weather_chain.run(weather=weather)\n",
        "    return outfit_chain.run(outfit=outfit)\n",
        "\n"
      ],
      "metadata": {
        "id": "uGpuySgsYsVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = get_outfit_recommendation(\"India\")\n",
        "result"
      ],
      "metadata": {
        "id": "7iTeZfBQayeA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5876bd1f-4086-4d47-8e2b-7a184b130ff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weather: The weather in India is Summer.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Bright colors and bold patterns are popular in Indian fashion, so don't be afraid to embrace them!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dynamic Function Generation"
      ],
      "metadata": {
        "id": "3B4DuypdcJP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "function_generator_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=\"Based on the following context, generate a JSON specification for a function that would be useful: {context}\\nEnsure the output is a single, valid JSON object.\" # Added instruction for valid JSON\n",
        ")\n",
        "\n",
        "function_generator_chain = LLMChain(llm=llm, prompt=function_generator_prompt)\n",
        "\n",
        "def generate_dynamic_function(context):\n",
        "    function_spec = function_generator_chain.run(context=context)\n",
        "    try:\n",
        "        # Attempt to parse JSON, handle errors gracefully\n",
        "        return json.loads(function_spec)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON: {e}\\nRaw output: {function_spec}\")  # Print error and raw output for debugging\n",
        "        return None  # Or handle the error differently\n",
        "\n",
        "context = \"The user is asking about financial calculations related to mortgages.\"\n",
        "dynamic_function = generate_dynamic_function(context)\n",
        "print(dynamic_function)"
      ],
      "metadata": {
        "id": "ftmnrgyta3iP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d243af04-540f-4e19-fe2b-93a2628c5f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'function_name': 'calculate_mortgage', 'parameters': [{'name': 'principal', 'type': 'number', 'description': 'The initial amount borrowed for the mortgage'}, {'name': 'interest_rate', 'type': 'number', 'description': 'The annual interest rate for the mortgage'}, {'name': 'loan_term', 'type': 'number', 'description': 'The length of the mortgage in years'}], 'return_type': 'object', 'description': 'Calculates various financial values related to a mortgage, including monthly payments, total interest paid, and total amount paid.', 'example': 'calculate_mortgage(200000, 4.5, 30) returns { monthly_payment: 1013.37, total_interest: 164813.42, total_amount: 364813.42 }'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Meta-Learning for Function Calling"
      ],
      "metadata": {
        "id": "I3M4BeB-Vdb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Assuming 'chat' is your ChatOpenAI instance\n",
        "llm = AzureOpenAI(deployment_name=\"dp-gpt-35-turbo-instruct\", model_name=\"gpt-35-turbo-instruct\")\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "def meta_function_caller(query, available_functions):\n",
        "    # 1. Provide context and ask for function choice\n",
        "    conversation.predict(\n",
        "        input=f\"Query: {query}\\nAvailable functions: {', '.join(available_functions)}\"\n",
        "    )\n",
        "    # 2. Get the function choice (extract from response if needed)\n",
        "    function_choice_response = conversation.predict(input=\"Which function should be called for this query?\")\n",
        "    function_choice = function_choice_response.strip() # Extract and clean the function name\n",
        "    # You might need more sophisticated logic to extract the function name reliably\n",
        "\n",
        "    # 3. Validate function choice (optional but recommended)\n",
        "    if function_choice not in available_functions:\n",
        "        print(f\"Warning: Invalid function choice '{function_choice}'.\")\n",
        "        # Handle invalid choice, e.g., reprompt or choose a default\n",
        "\n",
        "    return function_choice\n",
        "\n",
        "query = \"What is the stock  that i can buy for $1000 over 5 years at 5% annual rate?\"\n",
        "available_functions = [\"calculate_compound_interest\", \"get_stock_price\", \"convert_currency\"]\n",
        "chosen_function = meta_function_caller(query, available_functions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ydq1aRS5bj2c",
        "outputId": "96d7a1ba-820e-49d5-85f0-f14d23be6daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-72-b64ce99dceb4>:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "<ipython-input-72-b64ce99dceb4>:9: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Query: What is the stock  that i can buy for $1000 over 5 years at 5% annual rate?\n",
            "Available functions: calculate_compound_interest, get_stock_price, convert_currency\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Query: What is the stock  that i can buy for $1000 over 5 years at 5% annual rate?\n",
            "Available functions: calculate_compound_interest, get_stock_price, convert_currency\n",
            "AI:  The stock that you can buy for $1000 over 5 years at 5% annual rate is XYZ Inc. This stock has a current price of $50 per share and has been consistently growing at an average rate of 8% per year for the past 5 years. Based on these numbers, if you were to invest $1000 in XYZ Inc. and hold onto it for 5 years, you would have a total of 147 shares at the end of the 5 years. With the 5% annual rate, your initial investment of $1000 would have grown to $1,384.75. However, please keep in mind that stock prices can fluctuate and past performance does not guarantee future returns. Would you like me to calculate the compound interest and provide the final value for your investment?\n",
            "Human: Which function should be called for this query?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Warning: Invalid function choice 'The function that should be called for this query is calculate_compound_interest. This function takes in the initial investment, interest rate, and time period as parameters and returns the final value of the investment. In this case, the initial investment would be $1000, the interest rate would be 5%, and the time period would be 5 years. Would you like me to run this function for you now?'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v0F9nj1RoQr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Advanced Concepts in Function Calling"
      ],
      "metadata": {
        "id": "5ZXIp6mKWpNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-step Reasoning"
      ],
      "metadata": {
        "id": "zcQfO3WsWpom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "reasoning_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\", \"available_functions\"],\n",
        "    template=\"Query: {query}\\nAvailable functions: {available_functions}\\nBreak down the steps needed to answer this query using the available functions.\"\n",
        ")\n",
        "\n",
        "reasoning_chain = LLMChain(llm=llm, prompt=reasoning_prompt)\n",
        "\n",
        "def multi_step_function_calling(query, available_functions):\n",
        "    steps = reasoning_chain.run(query=query, available_functions=available_functions)\n",
        "    # Implement logic to execute each step and aggregate results\n",
        "    return steps\n",
        "\n",
        "query = \"What's the price difference between Apple and Microsoft stocks over the last month?\"\n",
        "available_functions = [\"get_stock_price\", \"calculate_percentage_change\", \"get_date_range\"]\n",
        "execution_plan = multi_step_function_calling(query, available_functions)\n",
        "execution_plan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "G2EibeWfWpv8",
        "outputId": "5676e9fd-f8a9-45fd-9104-1deadf3e1e83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nStep 1: Use the get_date_range function to get the date range of the last month.\\n\\nStep 2: Use the get_stock_price function to get the stock prices of Apple and Microsoft on the starting and ending dates of the last month.\\n\\nStep 3: Calculate the percentage change of Apple and Microsoft stocks using the calculate_percentage_change function.\\n\\nStep 4: Subtract the percentage change of Apple from the percentage change of Microsoft to find the price difference between the two stocks.\\n\\nStep 5: Print or display the result to the user.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **End Of Chapter 2**"
      ],
      "metadata": {
        "id": "ODQug3n0ZX_K"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}